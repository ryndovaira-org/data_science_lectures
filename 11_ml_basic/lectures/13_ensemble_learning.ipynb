{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Ансамбль методов (Ensemble learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Источники:**\n",
    "\n",
    "[Ансамбль методов (обучение машин)](https://ru.wikipedia.org/wiki/%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D1%8C_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%BE%D0%B2_(%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD))\n",
    "\n",
    "[Ensemble Models: Bagging & Boosting](https://medium.com/analytics-vidhya/ensemble-models-bagging-boosting-c33706db0b0b)\n",
    "\n",
    "[CatBoost Enables Fast Gradient Boosting on Decision Trees Using GPUs](https://catboost.ai/news/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus)\n",
    "\n",
    "[Ensemble methods: bagging, boosting and stacking](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n",
    "\n",
    "[Ensemble Learning: Bagging, Boosting & Stacking](https://www.kaggle.com/satishgunjal/ensemble-learning-bagging-boosting-stacking)\n",
    "\n",
    "[ENSEMBLE METHODS — Bagging, Boosting, and Stacking](https://medium.com/analytics-vidhya/ensemble-methods-bagging-boosting-and-stacking-28d006708731)\n",
    "\n",
    "[Bagging, Boosting, and Stacking in Machine Learning](https://dropsofai.com/bagging-boosting-and-stacking-in-machine-learning/)\n",
    "\n",
    "[Бустинг](https://ru.wikipedia.org/wiki/%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3)\n",
    "\n",
    "[Бэггинг](https://ru.wikipedia.org/wiki/%D0%91%D1%8D%D0%B3%D0%B3%D0%B8%D0%BD%D0%B3)\n",
    "\n",
    "[Ensemble Learning : Voting and Bagging](https://velog.io/@jiselectric/Ensemble-Learning-Voting-and-Bagging-at6219ae)\n",
    "\n",
    "[Bagging Classifier Python Code Example](https://vitalflux.com/bagging-classifier-python-code-example/)\n",
    "\n",
    "[Many Heads Are Better Than One: The Case For Ensemble Learning](https://www.kdnuggets.com/2019/09/ensemble-learning.html)\n",
    "\n",
    "[Guide To Ensemble Methods: Bagging vs Boosting](https://analyticsindiamag.com/guide-to-ensemble-methods-bagging-vs-boosting/)\n",
    "\n",
    "[Bagging, boosting and stacking in machine learning](https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning)\n",
    "\n",
    "[Stacking, make it better.](https://medium.com/@ryotennis0503/stacking-make-it-better-72a4ebee30ab)\n",
    "\n",
    "[A Practical Guide to Stacking Using Scikit-Learn](https://towardsdatascience.com/a-practical-guide-to-stacking-using-scikit-learn-91e8d021863d)\n",
    "\n",
    "[Why do stacked ensemble models win data science competitions?](https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/)\n",
    "\n",
    "[Stacking -Ensemble meta Algorithms for improve predictions](https://medium.com/ml-research-lab/stacking-ensemble-meta-algorithms-for-improve-predictions-f4b4cf3b9237)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning\n",
    "\n",
    "**Ансамбль методов** в статистике и машинном обучении использует **несколько обучающих алгоритмов (моделей) с целью получения лучшей эффективности, чем могли бы получить от каждого обучающего алгоритма (модель) по отдельности**. \n",
    "\n",
    "На практике, некоторые техники сборки в ансамбль (особенно **бэггинг**) склонны **уменьшить проблемы, связанные с переобучением** на тренировочных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap aggregating (Bagging/Бэггинг)\n",
    "\n",
    "Bagging = **B**ootstrap **agg**regat**ing**\n",
    "\n",
    "**Бутстрап-агрегирование или бэггинг**, это мета алгоритм композиционного машинного обучения (ансамбль методов), предназначенный для улучшения стабильности и точности алгоритмов машинного обучения, используемых в **классификации и регрессии**.\n",
    "\n",
    "Алгоритм также уменьшает дисперсию и **помогает избежать переобучения**. \n",
    "\n",
    "**Бутстрап-агрегирование, часто сокращаемое до бэггинг, даёт каждой модели в ансамбле одинаковый вес (голос).**\n",
    "\n",
    "Чтобы поддерживать вариантность, **бэггинг тренирует каждую модель в ансамбле с помощью случайно отобранного подмножества из тренировочного множества**. \n",
    "\n",
    "Как пример, алгоритм [Random forest](https://ru.wikipedia.org/wiki/Random_forest) комбинирует случайные [деревья решений](https://ru.wikipedia.org/wiki/%D0%94%D0%B5%D1%80%D0%B5%D0%B2%D0%BE_%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9) с бэггингом, чтобы получить высокую точность классификации.\n",
    "\n",
    "Бэггинг **обычно применяется** к моделям **на основе деревьев решений**, но его можно использовать с любым видом метода.\n",
    "\n",
    "**Обычно используются однородные \"слабые ученики\" (homogeneous weak learners).**\n",
    "\n",
    "Агрегация всех классификаторов происходит в виде **среднего значения или путем голосования**.\n",
    "\n",
    "<img src=\"images/bagging_1.png\" width=500>\n",
    "\n",
    "<img src=\"images/bagging_2.jpg\" width=400>\n",
    "\n",
    "**Принцип работы состоит в том, чтобы построить несколько базовых моделей независимо, а затем усреднить их для окончательного прогноза.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Бустинг строит ансамбль последовательными приращениями путём тренировки каждой новой модели, чтобы выделить тренировочные экземпляры, которые предыдущие модели классифицировали ошибочно.\n",
    "\n",
    "**В некоторых случаях бустинг даёт лучшие результаты, чем бэггинг, но имеет тенденцию к переобучению на тренировочных данных.**\n",
    "\n",
    "В то время как **бустинг алгоритмически не ограничен, большинство алгоритмов бустинга состоит из итеративного обучения слабых классификаторов с целью сборки их в сильный классификатор**.\n",
    "\n",
    "Когда они добавляются, им обычно приписываются некоторым образом **веса**, которые, обычно, **связаны с точностью обучения**.\n",
    "\n",
    "После того как слабый классификатор добавлен, **веса пересчитываются**, что известно как \"пересчёт весовых коэффициентов\".\n",
    "\n",
    "Неверно классифицированные входные данные получают больший вес, а правильно классифицированные экземпляры теряют вес.\n",
    "\n",
    "**Тем самым последующее слабое обучение фокусируется больше на примерах, где предыдущие слабые обучения дали ошибочную классификацию.**\n",
    "\n",
    "**Обычно используются однородные \"слабые ученики\" (homogeneous weak learners).**\n",
    "\n",
    "Существует множество алгоритмов бустинга.\n",
    "\n",
    "Наиболее частой реализацией бустинга являются алгоритмы [XGBoost](https://en.wikipedia.org/wiki/XGBoost) и [LightGBM](https://en.wikipedia.org/wiki/LightGBM).\n",
    "\n",
    "<img src=\"images/boosting_1.png\" width=400>\n",
    "\n",
    "<img src=\"images/boosting_2.jpg\" width=400>\n",
    "\n",
    "**Модели строятся последовательно и пытаются уменьшить систематическую ошибку в окончательных прогнозах.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Stacking создает ансамбль, который **объединяет выходные данные нескольких базовых моделей**.\n",
    "\n",
    "Модели базового уровня **обучаются на основе полного набора данных**, а затем их выходные данные используются в качестве входных функций для обучения функции ансамбля.\n",
    "\n",
    "Обычно функция ансамбля представляет собой простую линейную комбинацию оценок базовой модели.\n",
    "\n",
    "**Обычно используются НЕоднородные \"слабые ученики\" (heterogeneous weak learners), то есть различные модели.**\n",
    "\n",
    "**Прогнозы каждой отдельной модели складываются вместе и используются в качестве входных данных для окончательной оценки для вычисления прогноза.**\n",
    "\n",
    "<img src=\"images/stacking_1.png\" width=400>\n",
    "\n",
    "<img src=\"images/stacking_2.jpg\" width=400>\n",
    "\n",
    "<img src=\"images/deep_stacking_2.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging vs. Boosting vs. Stacking\n",
    "\n",
    "<img src=\"images/bagging_2.jpg\" width=320><img src=\"images/boosting_2.jpg\" width=320><img src=\"images/stacking_2.jpg\" width=320>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Какие еще бывают ансамбли?\n",
    "\n",
    "- Байесовский оптимальный классификатор (Bayes optimal classifier)\n",
    "- Усреднение байесовских параметров (Bayesian model averaging)\n",
    "- Комбинация байесовских моделей (Bayesian model combination)\n",
    "- Ведро моделей (Bucket of models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LevelUp_DataScience] *",
   "language": "python",
   "name": "conda-env-LevelUp_DataScience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}