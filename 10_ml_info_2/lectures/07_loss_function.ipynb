{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Функция потерь (Loss/Cost function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Источники:**\n",
    "\n",
    "[Функция потерь](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8C)\n",
    "\n",
    "[Loss function](https://en.wikipedia.org/wiki/Loss_function)\n",
    "\n",
    "[Функция потерь (Loss function)](https://wiki.loginom.ru/articles/loss-function.html)\n",
    "\n",
    "[Функция потерь и эмпирический риск](https://neerc.ifmo.ru/wiki/index.php?title=%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8C_%D0%B8_%D1%8D%D0%BC%D0%BF%D0%B8%D1%80%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D1%80%D0%B8%D1%81%D0%BA)\n",
    "\n",
    "[Cost Function is No Rocket Science!](https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/)\n",
    "\n",
    "[Функции оценки, потерь, оптимизации – основы алгоритма Машинного Обучения](https://id-lab.ru/posts/developers/funkcii/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание\n",
    "\n",
    "**Функция потерь (loss function / cost function / error function)** — функция, которая в теории статистических решений **характеризует потери при неправильном принятии решений на основе наблюдаемых данных**.\n",
    "\n",
    "В математической статистике и машинном обучении **функция потерь — это функция, которая отображает некоторое событие в виде действительного числа, интуитивно представляя некоторую \"стоимость\", связанную с событием**.\n",
    "\n",
    "Если решается задача оценки параметра сигнала на фоне помех, то функция потерь является **мерой расхождения между истинным значением оцениваемого параметра и оценкой параметра**.\n",
    "\n",
    "В статистике функция потерь обычно **используется для оценки параметров моделей**, а рассматриваемое событие **является разностью между оцененным и истинным значениями для каждого наблюдения набора данных**.\n",
    "\n",
    "**Задача оптимизации (например, градиентный спуск) стремится минимизировать функцию потерь.**\n",
    "\n",
    "Примеры:\n",
    "- В контексте экономики это обычно экономические издержки или потери.\n",
    "- В классификации это \"штраф\" модели за неправильное распознавание примера. \n",
    "- В управлении финансовыми рисками функция сопоставляется с денежными потерями.\n",
    "- Средняя квадратичная ошибка (MSE) для задач регрессии.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Виды функций потерь\n",
    "\n",
    "**На выбор функции потерь влияют особенности решаемой задачи.**\n",
    "\n",
    "Общего правила выбора функции потерь не существует.\n",
    "\n",
    "Обозначения:\n",
    "- $a$ - идеальная модель (функция).\n",
    "- $a(x)$ - истинное значение выходов модели (при обучении с учителем).\n",
    "- $y$ - реальная модель (функция).\n",
    "- $y(x)$ - фактическое значение выходов модели.\n",
    "- $L(a,x)$ - функция потерь.\n",
    "\n",
    "Чаще всего используются следующие функции потерь:\n",
    "- Простая (simple).\n",
    "    - $L(a, x)=a(x)-y(x)$\n",
    "    - Равна разности истинного и фактического выходов модели.\n",
    "    - Используется в тех случаях, где важен знак ошибки, например, при обучении нейронных сетей.\n",
    "\n",
    "\n",
    "- Двоичная / Пороговая функция потерь (0-1 loss function).\n",
    "    - $L(a,x)=[a(x) \\neq y(x)]$\n",
    "    - Используется в **бинарной классификации**.\n",
    "    - Потери определяются появлением двух взаимоисключающих состояний выхода модели.\n",
    "\n",
    "\n",
    "- Квадратичная.\n",
    "    - $L(a, x)=(a(x)-y(x))^2$\n",
    "    - Преимуществом квадратичной функции потерь являются инвариантность к знаку — значение функции всегда положительно. Т.е. независимо от знака ошибки результат будет один и тот же.\n",
    "    - Квадратичная функция потерь используется в моделях, параметры которых оцениваются на основе метода наименьших квадратов, например, линейной **регрессии**.\n",
    "\n",
    "\n",
    "- Логистическая.\n",
    "    - $L(a,x)=\\cfrac{ln(1+e^{-y(x)a(x)})}{ln2}$\n",
    "\n",
    "\n",
    "- Кросс-энтропия / Log loss.\n",
    "    - $t(y,x)=\\cfrac{1+y(x)}{2}$, $L(a,x)=-t*ln(a(x))-(1-t)*ln(1-a(x))$\n",
    "\n",
    "\n",
    "- Прямоугольная.\n",
    "\n",
    "\n",
    "- Экспоненциальная (функция потерь с насыщением).\n",
    "\n",
    "\n",
    "- Hinge loss.\n",
    "    - $L(a,x)=max(0,1-a(x)*y(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эмпирический риск (Empirical risk)\n",
    "\n",
    "Понятие функции потерь тесно связано с эмпирическим риском.\n",
    "\n",
    "**Эмпирический риск — средняя величина ошибки на обучающей выборке.**\n",
    "\n",
    "Как правило, **на тестовой выборке значение эмпирического риска будет больше, чем на обучающей выборке**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод минимизации эмпирического риска (МЭР, Empirical risk minimization, ERM)\n",
    "\n",
    "Логично предположить, что если алгоритм хорошо показывает себя на обучающей выборке, то и на реальных данных он будет работать неплохо.\n",
    "\n",
    "Так подходим к конструктивному методу обучения — методу минимизации эмпирического риска.\n",
    "\n",
    "**Метод простой, общий, конструктивный и зачастую сводит задачу обучения к численному поиску минимума в модели алгоритма.**\n",
    "\n",
    "Основное достоинство заключается в том, что это конструктивный и универсальный **подход, позволяющий сводить задачу обучения к задачам численной оптимизации**.\n",
    "\n",
    "Основной **недостаток — явление переобучения**, которое возникает практически всегда при использовании метода минимизации эмпирического риска.\n",
    "\n",
    "Оптимизаторы:\n",
    "\n",
    "- <img src=\"images/cost_function.png\" width=400>\n",
    "\n",
    "\n",
    "- В алгоритме **классического градиентного спуска** начинаем со случайных параметров модели и вычисляем ошибку для каждой итерации обучения, продолжая обновлять параметры, чтобы приблизиться к минимальным значениям.\n",
    "\n",
    "- **Пакетный (batch) градиентный спуск**: использует все обучающие данные для обновления параметров модели в каждой итерации.\n",
    "\n",
    "- **Мини-пакетный (mini-batch) градиентный спуск**: вместо использования всех данных, мини-пакетный градиентный спуск делит тренировочный набор на меньший размер, называемый партией, и обозначаемый буквой \"b\". Таким образом, мини-пакет \"b\" используется для обновления параметров модели на каждой итерации.\n",
    "\n",
    "\n",
    "- **Стохастический Градиентный Спуск (SGD)** обновляет параметры, используя только один обучающий параметр на каждой итерации. Такой параметр обычно выбирается случайным образом.\n",
    "\n",
    "- **Стохастический градиентный спуск** часто предпочтителен для оптимизации функций затрат, когда есть сотни тысяч обучающих или более параметров, поскольку он будет сходиться быстрее, чем пакетный градиентный спуск.\n",
    "\n",
    "\n",
    "- <img src=\"images/batch-stochastic-mini-gd.png\" height=300 width=300 />\n",
    "\n",
    "\n",
    "- <img src=\"images/batch-stochastic-mini-gd_in_one.png\" height=400 width=400 />\n",
    "\n",
    "\n",
    "- **AdaGrad (адаптивный градиентный алгоритм, adaptive gradient algorithm)** является модификацией стохастического алгоритма градиентного спуска с отдельной для каждого параметра скоростью обучения.\n",
    "\n",
    "\n",
    "- **RMSProp (Root Mean Square Propagation)** — это метод, в котором скорость обучения настраивается для каждого параметра.\n",
    "\n",
    "\n",
    "- **Adam (сокращение от \"метод адаптивной оценки моментов\", Adaptive Moment Estimation)**  — это обновление оптимизатора RMSProp.\n",
    "\n",
    "\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LevelUp_DataScience] *",
   "language": "python",
   "name": "conda-env-LevelUp_DataScience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
