{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Источники:**\n",
    "\n",
    "[XGBoost](https://neerc.ifmo.ru/wiki/index.php?title=XGBoost)\n",
    "\n",
    "[Виды ансамблей](https://neerc.ifmo.ru/wiki/index.php?title=%D0%92%D0%B8%D0%B4%D1%8B_%D0%B0%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B5%D0%B9)\n",
    "\n",
    "[XGBoost Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "\n",
    "[Эвристический алгоритм](https://ru.wikipedia.org/wiki/%D0%AD%D0%B2%D1%80%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC#:~:text=%D0%AD%D0%B2%D1%80%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%20(%D1%8D%D0%B2%D1%80%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B0)%20%E2%80%94%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC,%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BD%D0%B5%20%D0%BC%D0%BE%D0%B6%D0%B5%D1%82%20%D0%B1%D1%8B%D1%82%D1%8C%20%D0%BD%D0%B0%D0%B9%D0%B4%D0%B5%D0%BD%D0%BE.)\n",
    "\n",
    "[Настройка параметров XGBoost](https://russianblogs.com/article/2308221344)\n",
    "\n",
    "[Python API Reference](https://xgboost.readthedocs.io/en/latest/python/python_api.html)\n",
    "\n",
    "[Chapter 12 Gradient Boosting](https://bradleyboehmke.github.io/HOML/gbm.html)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Подготовка окружения"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ВНИМАНИЕ: необходимо удостовериться, что виртуальная среда выбрана правильно!\n",
    "\n",
    "# Для MacOS/Ubuntu\n",
    "# !which pip\n",
    "\n",
    "# Для Windows\n",
    "# !where pip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !conda install matplotlib numpy scikit-learn seaborn -y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Загрузка данных\n",
    "\n",
    "[Источник (FuelConsumption)](https://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./../../data/FuelConsumptionCo2.csv\")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Разделение данных"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['CO2EMISSIONS'].copy()\n",
    "X = df[['ENGINESIZE']].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Одна из самых популярных и эффективных реализаций алгоритма градиентного бустинга на деревьях на 2019-й год.**\n",
    "\n",
    "\n",
    "В основе XGBoost лежит алгоритм градиентного бустинга деревьев решений. \n",
    "\n",
    "**Градиентный бустинг** — это техника машинного обучения для задач классификации и регрессии, которая строит модель предсказания в форме ансамбля слабых предсказывающих моделей, обычно деревьев решений. Обучение ансамбля проводится последовательно в отличие, например от бэггинга. \n",
    "\n",
    "**Ансамбль алгоритмов (методов)** — метод, который использует несколько обучающих алгоритмов с целью получения лучшей эффективности прогнозирования, чем можно было бы получить от каждого обучающего алгоритма по отдельности.\n",
    "\n",
    "На каждой итерации вычисляются отклонения предсказаний уже обученного ансамбля на обучающей выборке. Следующая модель, которая будет добавлена в ансамбль будет предсказывать эти отклонения. Таким образом, добавив предсказания нового дерева к предсказаниям обученного ансамбля мы можем уменьшить среднее отклонение модели, котрое является таргетом оптимизационной задачи. Новые деревья добавляются в ансамбль до тех пор, пока ошибка уменьшается, либо пока не выполняется одно из правил \"ранней остановки\".\n",
    "\n",
    "\n",
    "XGBoost поддерживает все возможности таких библиотек как scikit-learn с возможностью добавлять регуляризацию. Поддержаны три главные формы градиетного бустинга:\n",
    "\n",
    "- Стандартный градиентный бустинг с возможностью изменения скорости обучения(learning rate).\n",
    "- Стохастический градиентный бустинг с возможностью семплирования по строкам и колонкам датасета.\n",
    "- Регуляризованный градиентный бустинг с L1 и L2 регуляризацией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделить независимую и зависимую переменные / train и test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['CO2EMISSIONS'].copy()\n",
    "X = df.drop(['CO2EMISSIONS', 'MAKE', 'MODEL', 'VEHICLECLASS', 'TRANSMISSION', 'FUELTYPE'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `class xgboost.XGBRegressor(**kwargs)`\n",
    "\n",
    "**[Остальные параметры см. 7_2_xgboost_params](./7_2_xgboost_params.ipynb)**\n",
    "\n",
    "- **`n_estimators (int)`**\n",
    "    - Количество деревьев с градиентным усилением. Эквивалентно количеству раундов boost-инга (boosting rounds).\n",
    "\n",
    "- **`max_depth (int) [default=6]`**\n",
    "    - Максимальная глубина дерева. Увеличение этого значения сделает модель более сложной и с большей вероятностью переобучится (overfit).\n",
    "    - Допустимые значения: $[0, \\infty]$, `0` принимается только в lossguided для роста, когда tree_method установлен как hist и указывает отсутствие ограничения на глубину. Помните, что XGBoost агрессивно расходует память при обучении глубокого дерева.\n",
    "\n",
    "- **`learning_rate (float) [default=0.3, alias: eta]`**\n",
    "    - Уменьшение размера шага, используемое в обновлении, чтобы **предотвратить переобучение (overfitting)**. После каждого шага повышения мы можем напрямую получить веса новых функций, а eta уменьшает веса функций, чтобы сделать процесс повышения более консервативным.\n",
    "    - Допустимые значения: $[0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-fe0d8d02009e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# импортировать пакет xgboost\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mxgboost\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mxg\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# создать объект XGBRegressor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mxgb_r\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mxg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mXGBRegressor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_estimators\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_depth\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# импортировать пакет xgboost\n",
    "import xgboost as xg\n",
    "\n",
    "# создать объект XGBRegressor\n",
    "xgb_r = xg.XGBRegressor(n_estimators = 10, max_depth=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit(X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)`\n",
    "\n",
    "**[Остальные параметры см. 7_2_xgboost_params](./7_2_xgboost_params.ipynb)**\n",
    "\n",
    "- **`X (array_like)`** - Матрица признаков.\n",
    "    \n",
    "    \n",
    "- **`y (array_like)`** - Labels (\"правильные ответы\").\n",
    "\n",
    "\n",
    "- **`early_stopping_rounds (int)`** \n",
    "    - Активирует раннюю остановку.\n",
    "    - Чтобы продолжить обучение, показатель валидации должен улучшаться хотя бы раз в каждом раунде `early_stopping_rounds`.\n",
    "    - Требуется хотя бы один элемент в `eval_set`.\n",
    "    - Метод возвращает модель с последней итерации (а не самой лучшей).\n",
    "    - Если в `eval_set` более одного элемента, для ранней остановки будет использоваться *последняя запись* (last entry).\n",
    "    - Если в `eval_metric` более одной метрики, для ранней остановки будет использоваться *последняя метрика* (last metric).\n",
    "    - Если произойдет ранняя остановка, модель будет иметь три дополнительных поля: `clf.best_score`, `clf.best_iteration` и `clf.best_ntree_limit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучить модель\n",
    "xgb_r.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества модели (Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `predict(data, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None)`\n",
    "\n",
    "**[Остальные параметры см. 7_2_xgboost_params](./7_2_xgboost_params.ipynb)**\n",
    "\n",
    "- **`data (numpy.array/scipy.sparse)`**\n",
    "    - Данные для прогнозирования.\n",
    "    \n",
    "    \n",
    "- **`validate_features (bool)`**\n",
    "    - Если это `True`, то будет произведена проверка (validate), что `feature_names` для `Booster` и `data` идентичны. В противном случае предполагается, что `feature_names` совпадают."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# использовать обученную модель для предсказания на test выборке\n",
    "y_predicted = xgb_r.predict(X_test)\n",
    "\n",
    "# вывести результаты предсказания\n",
    "print(f'Variance score: {xgb_r.score(X_test, y_test)}')    # Coefficient of determination R^2 of the prediction\n",
    "print(f\"Residual sum of squares: {np.mean((y_predicted - y_test) ** 2)}\")    # MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_true=y_test, y_pred=y_predicted)    # эквивалентно simple_regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# MSE\n",
    "mean_squared_error(y_test, y_predicted, squared=True)   # эквивалентно np.mean((y_predicted - y_test) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Параметры XGBRegressor для Python Scikit-Learn API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `class xgboost.XGBRegressor(**kwargs)`\n",
    "\n",
    "- **`n_estimators (int)`**\n",
    "    - Количество деревьев с градиентным усилением. Эквивалентно количеству раундов boost-инга (boosting rounds).\n",
    "\n",
    "\n",
    "- **`max_depth (int) [default=6]`**\n",
    "    - Максимальная глубина дерева. Увеличение этого значения сделает модель более сложной и с большей вероятностью переобучится (overfit).\n",
    "    - Допустимые значения: $[0, \\infty]$, `0` принимается только в `lossguided` для роста, когда `tree_method` установлен как hist и указывает отсутствие ограничения на глубину. Помните, что XGBoost агрессивно расходует память при обучении глубокого дерева.\n",
    "\n",
    "\n",
    "- **`learning_rate (float) [default=0.3, alias: eta]`**\n",
    "    - Уменьшение размера шага, используемое в обновлении, чтобы **предотвратить переобучение (overfitting)**. После каждого шага повышения мы можем напрямую получить веса новых функций, а eta уменьшает веса функций, чтобы сделать процесс повышения более консервативным.\n",
    "    - Допустимые значения: $[0, 1]$\n",
    "\n",
    "\n",
    "- **`subsample (float) [default=1]`**\n",
    "    - Соотношение подвыборки обучающих примеров. Установка его на `0.5` означает, что XGBoost будет случайным образом выбирать половину обучающих данных перед выращиванием деревьев. И это *предотвратит переобучение (overfitting)*.\n",
    "    - Подвыборка (subsample) будет выполняться один раз на каждой итерации boosting-а.\n",
    "    - Допустимые значения: $(0, 1]$\n",
    "\n",
    "\n",
    "- `verbosity  (int) [default=1]`\n",
    "    - Детальность печатных сообщений.\n",
    "    - Допустимые значения:\n",
    "        - 0 (silent)\n",
    "        - 1 (warning)\n",
    "        - 2 (info)\n",
    "        - 3 (debug)\n",
    "    - Иногда XGBoost пытается изменить конфигурации на основе эвристики, что отображается как предупреждающее сообщение. Если происходит неожиданное поведение, попробуйте увеличить значение подробности.\n",
    "    - *Эвристический алгоритм (эвристика) — алгоритм решения задачи, включающий практический метод, не являющийся гарантированно точным или оптимальным, но достаточный для решения поставленной задачи. Позволяет ускорить решение задачи в тех случаях, когда точное решение не может быть найдено.\n",
    "\n",
    "\n",
    "- `objective (string | callable)`\n",
    "    - Устанавливает задачу обучения (learning task) и соответствующую цель обучения (learning objective) или настраиваемую целевую функцию (objective function), которая будет использоваться.\n",
    "    - В этом случае objective parameter должен иметь сигнатуру `(y_true, y_pred) -> grad, hess`\n",
    "        - `y_true: array_like of shape [n_samples]` - Целевые значения\n",
    "        - `y_pred: array_like of shape [n_samples]` - Прогнозируемые значения\n",
    "        - `grad: array_like of shape [n_samples]`- Значение градиента для каждой точки выборки.\n",
    "        - `hess: array_like of shape [n_samples]` - Значение второй производной для каждой точки выборки\n",
    "\n",
    "\n",
    "- `booster (string) [default='gbtree']`\n",
    "    - Какой booster использовать.\n",
    "    - Допустимые значения:\n",
    "        - `gbtree`\n",
    "        - `gblinear`\n",
    "        - `dart`\n",
    "    - `gbtree` и `dart` используют модели на основе дерева (tree based models), в то время как `gblinear` использует линейные функции (linear functions).\n",
    "\n",
    "\n",
    "- `tree_method (string) [default='auto']`\n",
    "    - Алгоритм построения дерева, используемый в XGBoost.\n",
    "    - XGBoost поддерживает `approx`, `hist` и `gpu_hist` для распределенного обучения. Экспериментальная поддержка внешней памяти доступна для `approx` и `gpu_hist`.\n",
    "    - Допустимые значения: `auto`, `exact`, `approx`, `hist`, `gpu_hist`, это комбинация часто используемых средств обновления. Для других средств обновления, таких как `refresh`, установите параметр `updater` напрямую.\n",
    "        - `auto`: Использует эвристику, чтобы выбрать самый быстрый метод.\n",
    "            - Для большего набора данных будет выбран `approx`. Рекомендуется попробовать `hist` и `gpu_hist` для повышения производительности с большим набором данных. `gpu_hist` поддерживает внешнюю память.\n",
    "            - Поскольку старое поведение всегда использует точную жадность ([exact greedy](https://medium.com/analytics-vidhya/boosting-models-unwrapping-the-basic-exact-greedy-algorithm-b67d88c7189a)) на одной машине, пользователь получит сообщение, когда будет выбран `approx` алгоритм для уведомления об этом выборе.\n",
    "        - `exact`: Точный жадный алгоритм ([exact greedy](https://medium.com/analytics-vidhya/boosting-models-unwrapping-the-basic-exact-greedy-algorithm-b67d88c7189a)). Перечисляет всех отдельных кандидатов.\n",
    "        - `approx`: Приблизительный жадный алгоритм ([approximate greedy](https://towardsdatascience.com/why-xgboost-is-so-effective-3a193951e289)) с использованием квантилей ([quantile sketch](https://datasketches.apache.org/docs/Quantiles/QuantilesOverview.html)) и гистограммы градиента.\n",
    "        - `hist`: Более быстрый приближенный жадный алгоритм Приблизительный жадный алгоритм ([approximate greedy](https://towardsdatascience.com/why-xgboost-is-so-effective-3a193951e289)), оптимизированный для гистограммы.\n",
    "        - `gpu_hist`: Реализация алгоритма `hist` на GPU.\n",
    "\n",
    "\n",
    "- `n_jobs (int)`\n",
    "    - Количество параллельных потоков, используемых для запуска `xgboost`.\n",
    "    - При использовании с другими алгоритмами Scikit-Learn, такими как `grid search`, можно выбрать, какой алгоритм распараллеливать и балансировать потоки.\n",
    "    - Создание конкуренции потоков ([thread contention](https://stackoverflow.com/questions/1970345/what-is-thread-contention#:~:text=Essentially%20thread%20contention%20is%20a,has%20unlocked%20that%20particular%20object.)) значительно замедлит работу обоих алгоритмов.\n",
    "\n",
    "\n",
    "- `gamma (float) [default=0, alias: min_split_loss]`\n",
    "    - Минимальное изменение значения функции потерь (Minimum loss reduction), необходимое для дальнейшего разбиения листового узла (leaf node) дерева (tree), то есть разделение листа на поддеревья. Чем больше `gamma`, тем более \"консервативным\" будет алгоритм.\n",
    "    - Допустимые значения: $[0, \\infty]$\n",
    "\n",
    "\n",
    "- `min_child_weight (float) [default=1]`\n",
    "    - Минимальная сумма веса экземпляра (hessian), необходимая для дочернего узла (child). Если на этапе разбиения дерева получается листовой узел (leaf node) с суммой веса экземпляра меньше min_child_weight, то процесс построения откажется от дальнейшего разделения.\n",
    "    - В задаче линейной регрессии (linear regression) это просто соответствует минимальному количеству экземпляров (instances), которое должно быть в каждом узле. Чем больше min_child_weight, тем более \"консервативным\": будет алгоритм.\n",
    "    - Допустимые значения: $[0, \\infty]$\n",
    "\n",
    "\n",
    "- `max_delta_step (int) [default=0]`\n",
    "    - Максимальный шаг дельты, который допускается на выходе каждого листа (leaf output) для оценки веса каждого дерева.\n",
    "    - Если значение установлено на `0`, это означает, что ограничения нет.\n",
    "    - Если для него установлено `положительное значение`, это может помочь сделать шаг обновления более \"консервативным\".\n",
    "    - *Обычно этот параметр не нужен*, но он может помочь в логистической регрессии (logistic regression), когда класс крайне несбалансирован (imbalanced).\n",
    "    - Установка значения 1-10 может помочь контролировать обновление.\n",
    "    - Допустимые значения: $[0, 1]$\n",
    "\n",
    "\n",
    "- `colsample_bytree, colsample_bylevel, colsample_bynode (float)[default=1]`\n",
    "    - Это семейство параметров для подвыборки столбцов.\n",
    "    - Все параметры `colsample_by*` имеют диапазон `(0, 1]`, значение по умолчанию 1, и определяют долю столбцов, подлежащих подвыборке.\n",
    "    - `colsample_bytree` - это соотношение столбцов подвыборки при построении каждого дерева. Подвыборка (subsampling) выполняется один раз для каждого построенного дерева.\n",
    "    - `colsample_bylevel` - отношение подвыборки столбцов для каждого уровня. Подвыборка (subsampling) выполняется один раз для каждого нового уровня глубины, достигнутого в дереве. Столбцы выбираются из набора столбцов, выбранных для *текущего дерева*.\n",
    "    - `colsample_bynode` - это отношение подвыборки столбцов для каждого узла (разбиение). Подвыборка (subsampling) происходит каждый раз при оценке нового разделения. Столбцы выбираются из набора столбцов, выбранных для *текущего уровня*.\n",
    "    - `colsample_by*` параметры работают [кумулятивно (накопительно)](https://ru.wiktionary.org/wiki/%D0%BA%D1%83%D0%BC%D1%83%D0%BB%D1%8F%D1%82%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9). Например, комбинация `{'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5}` с 64 признаками оставит 8 признаков на выбор в каждом сплите (split).\n",
    "    - В интерфейсе Python при использовании метода `hist`, `gpu_hist` или `exact` методов дерева (tree method) можно установить `feature_weights` для DMatrix, чтобы определить вероятность выбора каждой функции при использовании выборки столбцов (column sampling). Аналогичный параметр для метода `fit` есть в интерфейсе sklearn.\n",
    "\n",
    "\n",
    "- `reg_alpha (float) [default=0, alias: alpha]`\n",
    "    - Элемент регуляризации L1 ([lasso regression](http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%B0%D1%81%D1%81%D0%BE)) на весах.\n",
    "    - Увеличение этого значения сделает модель более \"консервативной\".\n",
    "\n",
    "- `reg_lambda (float) [default=1, alias: lambda]`\n",
    "    - Условие L2 регуляризации ([ridge regression или Tikhonov regularization](http://www.machinelearning.ru/wiki/index.php?title=%D0%A0%D0%B8%D0%B4%D0%B6-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F)) весов.\n",
    "    - Увеличение этого значения сделает модель более \"консервативной\".\n",
    "\n",
    "\n",
    "- `scale_pos_weight (float) [default=1]`\n",
    "    - Контролирует баланс положительных и отрицательных весов, что полезно для несбалансированных классов.\n",
    "    - Типичное значение, которое следует учитывать: `sum(negative instances) / sum(positive instances)`.\n",
    "\n",
    "\n",
    "- `base_score (float) [default=0.5]`\n",
    "    - Начальная оценка прогноза для всех экземпляров, глобальный bias.\n",
    "    - При достаточном количестве итераций изменение этого значения не будет иметь большого эффекта.\n",
    "\n",
    "\n",
    "- `random_state (int)`\n",
    "    - Начальное число (порождающий элемент) для генератора случайных чисел ([random seed](https://en.wikipedia.org/wiki/Random_seed)).\n",
    "    - xgboost использует генератор случайных чисел (random generator) только для подвыборки (subsampling). В остальном поведение детерминировано.\n",
    "\n",
    "\n",
    "- `missing (float) [default=np.nan]`\n",
    "    - Значение, которое должны быть использовано в качестве пропущенных значений (missing value).\n",
    "\n",
    "\n",
    "- `num_parallel_tree (int) [default=1] `\n",
    "    - Количество параллельных деревьев (trees), построенных на каждой итерации.\n",
    "    - Эта опция используется для поддержки boosting-а случайного леса (random forest).\n",
    "\n",
    "\n",
    "- `monotone_constraints (str)`\n",
    "    - Ограничение переменной монотонности.\n",
    "\n",
    "\n",
    "- `interaction_constraints (str)`\n",
    "    - Ограничение для взаимодействия, представляющие разрешенные взаимодействия.\n",
    "    - Ограничения должны быть указаны в виде списка вложений, например `[[0, 1], [2, 3, 4]]`, где каждый внутренний список представляет собой группу индексов функций, которым разрешено взаимодействовать друг с другом.\n",
    "\n",
    "\n",
    "- `importance_type (string) [default=\"gain\"]`\n",
    "    - Тип важности признака для свойства `feature_importances_`:.\n",
    "    - Допустимые значения: `\"gain\"`, `\"weight\"`, `\"cover\"`, `\"total_gain\"` или `\"total_cover\"`.\n",
    "\n",
    "\n",
    "- `**kwargs (dict, optional)`\n",
    "    - `**kwargs` для объекта XGBoost Booster. Полную документацию по параметрам можно найти [здесь](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst).\n",
    "    - `**kwargs` не поддерживается scikit-learn. Нет  гарантии, что параметры, переданные через этот аргумент, будут правильно взаимодействовать с scikit-learn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `fit(X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)`\n",
    "\n",
    "Многократный вызов `fit()` приведет к повторному обучению модели с нуля (re-fit from scratch).\n",
    "\n",
    "Чтобы возобновить обучение с предыдущей контрольной точки, явно передайте аргумент xgb_model.\n",
    "\n",
    "\n",
    "- **`X (array_like)`** - Матрица признаков.\n",
    "\n",
    "\n",
    "- **`y (array_like)`** - Labels (\"правильные ответы\").\n",
    "\n",
    "\n",
    "- **`early_stopping_rounds (int)`**\n",
    "    - Активирует раннюю остановку.\n",
    "    - Чтобы продолжить обучение, показатель валидации должен улучшаться хотя бы раз в каждом раунде `early_stopping_rounds`.\n",
    "    - Требуется хотя бы один элемент в `eval_set`.\n",
    "    - Метод возвращает модель с последней итерации (а не самой лучшей).\n",
    "    - Если в `eval_set` более одного элемента, для ранней остановки будет использоваться *последняя запись* (last entry).\n",
    "    - Если в `eval_metric` более одной метрики, для ранней остановки будет использоваться *последняя метрика* (last metric).\n",
    "    - Если произойдет ранняя остановка, модель будет иметь три дополнительных поля: `clf.best_score`, `clf.best_iteration` и `clf.best_ntree_limit`.\n",
    "\n",
    "\n",
    "- `sample_weight (array_like)` - Веса экземпляров.\n",
    "\n",
    "\n",
    "- `base_margin (array_like)` - Глобальный байес (bias) для каждого экземпляра.\n",
    "\n",
    "\n",
    "- `eval_set (list, optional)` - Список пар кортежей (X, y) для использования в качестве проверочных наборов (validation sets), для которых будут вычисляться метрики. Показатели валидации (Validation metrics) помогают отслеживать производительность (performance) модели.\n",
    "\n",
    "\n",
    "- `eval_metric (str, list of str, or callable, optional)`\n",
    "    - Если `str`, должна быть встроенная (built-in) метрика оценки для использования.\n",
    "    - Если список `list of str`, должен быть список нескольких встроенных метрик оценки для использования.\n",
    "    - Если вызываемая (`callable`), то пользовательская метрика оценки. Сигнатура вызова - `func(y_predicted, y_true)`, где `y_true` будет объектом `DMatrix`, может потребоваться вызвать метод `get_label`. Он должен возвращать пару `str, value`, где `str` - это имя для оценки, а `value` - это значение функции оценки. Вызываемая пользовательская задача (objective) всегда сводится к минимуму."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `predict(data, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None)`\n",
    "\n",
    "**Эта функция не является потокобезопасной.**\n",
    "\n",
    "Для каждого объекта booster-а `predict` можно вызвать только из одного потока.\n",
    "\n",
    "Если необходимо выполнить `predict` с использованием нескольких потоков, используйте `xgb.copy()`, чтобы сделать копии объекта модели, а затем вызовите `predict()`.\n",
    "\n",
    "- **`data (numpy.array/scipy.sparse)`**\n",
    "    - Данные для прогнозирования.\n",
    "\n",
    "\n",
    "- **`validate_features (bool)`**\n",
    "    - Если это `True`, то будет произведена проверка (validate), что `feature_names` для `Booster` и `data` идентичны. В противном случае предполагается, что `feature_names` совпадают.\n",
    "\n",
    "\n",
    "- `output_margin (bool)`\n",
    "    - Выводить ли необработанное непреобразованное значение границы (margin).\n",
    "\n",
    "\n",
    "- `ntree_limit (int)`\n",
    "    - Ограничить количество деревьев в прогнозе\n",
    "    - Значение по умолчанию: `best_ntree_limit`, если он определен (т.е. он был обучен с ранней остановкой (early stopping)), в противном случае - `0` (использовать все деревья)."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}