{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Параметры XGBRegressor (XGBRegressor Parameters)\n",
    "\n",
    "Интерфейс обертки Scikit-Learn для XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `class xgboost.XGBRegressor(**kwargs)`\n",
    "\n",
    "- **`n_estimators (int)`**\n",
    "    - Количество деревьев с градиентным усилением. Эквивалентно количеству раундов boost-инга (boosting rounds).\n",
    "\n",
    "\n",
    "- **`max_depth (int) [default=6]`**\n",
    "    - Максимальная глубина дерева. Увеличение этого значения сделает модель более сложной и с большей вероятностью переобучится (overfit).\n",
    "    - Допустимые значения: $[0, \\infty]$, `0` принимается только в lossguided для роста, когда tree_method установлен как hist и указывает отсутствие ограничения на глубину. Помните, что XGBoost агрессивно расходует память при обучении глубокого дерева.\n",
    "\n",
    "\n",
    "- **`learning_rate (float) [default=0.3, alias: eta]`**\n",
    "    - Уменьшение размера шага, используемое в обновлении, чтобы **предотвратить переобучение (overfitting)**. После каждого шага повышения мы можем напрямую получить веса новых функций, а eta уменьшает веса функций, чтобы сделать процесс повышения более консервативным.\n",
    "    - Допустимые значения: $[0, 1]$\n",
    "\n",
    "\n",
    "- `verbosity  (int) [default=1]`\n",
    "    - Детальность печатных сообщений.\n",
    "    - Допустимые значения:\n",
    "        - 0 (silent)\n",
    "        - 1 (warning)\n",
    "        - 2 (info)\n",
    "        - 3 (debug)\n",
    "    - Иногда XGBoost пытается изменить конфигурации на основе эвристики, что отображается как предупреждающее сообщение. Если происходит неожиданное поведение, попробуйте увеличить значение подробности.\n",
    "    - *Эвристический алгоритм (эвристика) — алгоритм решения задачи, включающий практический метод, не являющийся гарантированно точным или оптимальным, но достаточный для решения поставленной задачи. Позволяет ускорить решение задачи в тех случаях, когда точное решение не может быть найдено.\n",
    "\n",
    "\n",
    "- `objective (string | callable)`\n",
    "    - Устанавливает задачу обучения (learning task) и соответствующую цель обучения (learning objective) или настраиваемую целевую функцию (objective function), которая будет использоваться.\n",
    "    - В этом случае objective parameter должен иметь сигнатуру `(y_true, y_pred) -> grad, hess`\n",
    "        - `y_true: array_like of shape [n_samples]` - Целевые значения\n",
    "        - `y_pred: array_like of shape [n_samples]` - Прогнозируемые значения\n",
    "        - `grad: array_like of shape [n_samples]`- Значение градиента для каждой точки выборки.\n",
    "        - `hess: array_like of shape [n_samples]` - Значение второй производной для каждой точки выборки\n",
    "\n",
    "\n",
    "- `booster (string) [default='gbtree']`\n",
    "    - Какой booster использовать.\n",
    "    - Допустимые значения:\n",
    "        - `gbtree`\n",
    "        - `gblinear`\n",
    "        - `dart`\n",
    "    - `gbtree` и `dart` используют модели на основе дерева (tree based models), в то время как `gblinear` использует линейные функции (linear functions).\n",
    "\n",
    "\n",
    "- `tree_method (string) [default='auto']`\n",
    "    - Алгоритм построения дерева, используемый в XGBoost.\n",
    "    - XGBoost поддерживает `approx`, `hist` и `gpu_hist` для распределенного обучения. Экспериментальная поддержка внешней памяти доступна для `approx` и `gpu_hist`.\n",
    "    - Допустимые значения: `auto`, `exact`, `approx`, `hist`, `gpu_hist`, это комбинация часто используемых средств обновления. Для других средств обновления, таких как `refresh`, установите параметр `updater` напрямую.\n",
    "        - `auto`: Использует эвристику, чтобы выбрать самый быстрый метод.\n",
    "            - Для большего набора данных будет выбран `approx`. Рекомендуется попробовать `hist` и `gpu_hist` для повышения производительности с большим набором данных. `gpu_hist` поддерживает внешнюю память.\n",
    "            - Поскольку старое поведение всегда использует точную жадность ([exact greedy](https://medium.com/analytics-vidhya/boosting-models-unwrapping-the-basic-exact-greedy-algorithm-b67d88c7189a)) на одной машине, пользователь получит сообщение, когда будет выбран `approx` алгоритм для уведомления об этом выборе.\n",
    "        - `exact`: Точный жадный алгоритм ([exact greedy](https://medium.com/analytics-vidhya/boosting-models-unwrapping-the-basic-exact-greedy-algorithm-b67d88c7189a)). Перечисляет всех отдельных кандидатов.\n",
    "        - `approx`: Приблизительный жадный алгоритм ([approximate greedy](https://towardsdatascience.com/why-xgboost-is-so-effective-3a193951e289)) с использованием квантилей ([quantile sketch](https://datasketches.apache.org/docs/Quantiles/QuantilesOverview.html)) и гистограммы градиента.\n",
    "        - `hist`: Более быстрый приближенный жадный алгоритм Приблизительный жадный алгоритм ([approximate greedy](https://towardsdatascience.com/why-xgboost-is-so-effective-3a193951e289)), оптимизированный для гистограммы.\n",
    "        - `gpu_hist`: Реализация алгоритма `hist` на GPU.\n",
    "\n",
    "- `n_jobs (int)` \n",
    "    - Количество параллельных потоков, используемых для запуска `xgboost`.\n",
    "    - При использовании с другими алгоритмами Scikit-Learn, такими как `grid search`, можно выбрать, какой алгоритм распараллеливать и балансировать потоки.\n",
    "    - Создание конкуренции потоков ([thread contention](https://stackoverflow.com/questions/1970345/what-is-thread-contention#:~:text=Essentially%20thread%20contention%20is%20a,has%20unlocked%20that%20particular%20object.)) значительно замедлит работу обоих алгоритмов.\n",
    "\n",
    "- `gamma (float) [default=0, alias: min_split_loss]`\n",
    "    - Минимальное изменение значения функции потерь (Minimum loss reduction), необходимое для дальнейшего разбиения листового узла (leaf node) дерева (tree), то есть разделение листа на поддеревья. Чем больше `gamma`, тем более \"консервативным\" будет алгоритм.\n",
    "    - Допустимые значения: $[0, \\infty]$\n",
    "\n",
    "- `min_child_weight (float) [default=1]`\n",
    "    - Минимальная сумма веса экземпляра (hessian), необходимая для дочернего узла (child). Если на этапе разбиения дерева получается листовой узел (leaf node) с суммой веса экземпляра меньше min_child_weight, то процесс построения откажется от дальнейшего разделения.\n",
    "    - В задаче линейной регрессии (linear regression) это просто соответствует минимальному количеству экземпляров (instances), которое должно быть в каждом узле. Чем больше min_child_weight, тем более \"консервативным: будет алгоритм.\n",
    "    - Допустимые значения: $[0, \\infty]$\n",
    "    \n",
    "\n",
    "- `max_delta_step (int) [default=0]`\n",
    "    - Максимальный шаг дельты, который допускается на выходе каждого листа (leaf output) для оценки веса каждого дерева.\n",
    "    - Если значение установлено на `0`, это означает, что ограничения нет.\n",
    "    - Если для него установлено `положительное значение`, это может помочь сделать шаг обновления более \"консервативным\".\n",
    "    - *Обычно этот параметр не нужен*, но он может помочь в логистической регрессии (logistic regression), когда класс крайне несбалансирован (imbalanced).\n",
    "    - Установка значения 1-10 может помочь контролировать обновление.\n",
    "    - Допустимые значения: $[0, 1]$    \n",
    "    \n",
    "    \n",
    "- **`subsample (float) [default=1]`**\n",
    "    - Соотношение подвыборки обучающих примеров. Установка его на `0.5` означает, что XGBoost будет случайным образом выбирать половину обучающих данных перед выращиванием деревьев. И это *предотвратит переобучение (overfitting)*.\n",
    "    - Подвыборка (subsample) будет выполняться один раз на каждой итерации boosting'а.\n",
    "    - Допустимые значения: $(0, 1]$\n",
    "    \n",
    "- `colsample_bytree, colsample_bylevel, colsample_bynode (float)[default=1]`    \n",
    "    - Это семейство параметров для подвыборки столбцов.\n",
    "    - Все параметры `colsample_by*` имеют диапазон `(0, 1]`, значение по умолчанию 1, и определяют долю столбцов, подлежащих подвыборке.\n",
    "    - `colsample_bytree` - это соотношение столбцов подвыборки при построении каждого дерева. Подвыборка (subsampling) выполняется один раз для каждого построенного дерева.\n",
    "    - `colsample_bylevel` - отношение подвыборки столбцов для каждого уровня. Подвыборка (subsampling) выполняется один раз для каждого нового уровня глубины, достигнутого в дереве. Столбцы выбираются из набора столбцов, выбранных для *текущего дерева*.\n",
    "    - `colsample_bynode` - это отношение подвыборки столбцов для каждого узла (разбиение). Подвыборка (subsampling) происходит каждый раз при оценке нового разделения. Столбцы выбираются из набора столбцов, выбранных для *текущего уровня*.\n",
    "    - `colsample_by*` параметры работают [кумулятивно (накопительно)](https://ru.wiktionary.org/wiki/%D0%BA%D1%83%D0%BC%D1%83%D0%BB%D1%8F%D1%82%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9). Например, комбинация `{'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5}` с 64 признаками оставит 8 признаков на выбор в каждом сплите (split).\n",
    "    - В интерфейсе Python при использовании метода `hist`, `gpu_hist` или `exact` методов дерева (tree method) можно установить `feature_weights` для DMatrix, чтобы определить вероятность выбора каждой функции при использовании выборки столбцов (column sampling). Аналогичный параметр для метода `fit` есть в интерфейсе sklearn.\n",
    "    \n",
    "- `reg_alpha (float) [default=0, alias: alpha]`\n",
    "    - Элемент регуляризации L1 ([lasso regression](http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%B0%D1%81%D1%81%D0%BE)) на весах.\n",
    "    - Увеличение этого значения сделает модель более \"консервативной\".    \n",
    "    \n",
    "- `reg_lambda (float) [default=1, alias: lambda]`\n",
    "    - Условие L2 регуляризации ([ridge regression или Tikhonov regularization](http://www.machinelearning.ru/wiki/index.php?title=%D0%A0%D0%B8%D0%B4%D0%B6-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F)) весов.\n",
    "    - Увеличение этого значения сделает модель более \"консервативной\".\n",
    "\n",
    "- `scale_pos_weight (float) [default=1]`\n",
    "    - Контролирует баланс положительных и отрицательных весов, что полезно для несбалансированных классов.\n",
    "    - Типичное значение, которое следует учитывать: `sum(negative instances) / sum(positive instances)`.\n",
    "    \n",
    "- `base_score (float) [default=0.5]`\n",
    "    - Начальная оценка прогноза для всех экземляров, глобальный bias.\n",
    "    - При достаточном количестве итераций изменение этого значения не будет иметь большого эффекта.\n",
    "\n",
    "- `random_state (int)`\n",
    "    - Начальное число (порождающий элемент) для генератора случайных чисел ([random seed](https://en.wikipedia.org/wiki/Random_seed)).\n",
    "    - xgboost использует генератор случайных чисел (random generator) только для подвыборки (subsampling). В остальном поведение детерминировано.\n",
    "    \n",
    "- `missing (float) [default=np.nan]`\n",
    "    - Значение, которое должны быть использвано в качестве пропущенных значений (missing value).\n",
    "    \n",
    "- `num_parallel_tree (int) [default=1] `\n",
    "    - Количество параллельных деревьев (trees), построенных на каждой итерации.\n",
    "    - Эта опция используется для поддержки boost'инга случайного леса (random forest).\n",
    "    \n",
    "    \n",
    "- `monotone_constraints (str)`\n",
    "    - Ограничение переменной монотонности.\n",
    "\n",
    "- `interaction_constraints (str)`\n",
    "    - Ограничение для взаимодействия, представляющие разрешенные взаимодействия.\n",
    "    - Ограничения должны быть указаны в виде списка вложений, например `[[0, 1], [2, 3, 4]]`, где каждый внутренний список представляет собой группу индексов функций, которым разрешено взаимодействовать друг с другом.\n",
    "    \n",
    "- `importance_type (string) [default=\"gain\"]`\n",
    "    - Тип важности признака для свойства `feature_importances_`:.\n",
    "    - Допустимые значения: `\"gain\"`, `\"weight\"`, `\"cover\"`, `\"total_gain\"` или `\"total_cover\"`.\n",
    "    \n",
    "    \n",
    "- `**kwargs (dict, optional)`\n",
    "    - `**kwargs` для объекта XGBoost Booster. Полную документацию по параметрам можно найти [здесь](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst).\n",
    "    - `**kwargs` не поддерживается scikit-learn. Нет гарантирии, что параметры, переданные через этот аргумент, будут правильно взаимодействовать с scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit(X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)`\n",
    "\n",
    "Многократный вызов `fit()` приведет к повторному обучению модели с нуля (re-fit from scratch).\n",
    "\n",
    "Чтобы возобновить обучение с предыдущей контрольной точки, явно передайте аргумент xgb_model.\n",
    "\n",
    "- `X (array_like)` - Матрица признаков.\n",
    "    \n",
    "- `y (array_like)` - Labels (\"правильные ответы\").\n",
    "    \n",
    "- `sample_weight (array_like)` - Веса экземпляров.\n",
    "\n",
    "- `base_margin (array_like)` - Глобальный байес (bias) для каждого экземпляра. \n",
    "\n",
    "- `eval_set (list, optional)` - Список пар кортежей (X, y) для использования в качестве проверочных наборов (validation sets), для которых будут вычисляться метрики. Показатели валидации (Validation metrics) помогают отслеживать производительность (performance) модели.\n",
    "\n",
    "- `eval_metric (str, list of str, or callable, optional)` \n",
    "    - Если `str`, должна быть встроенная (built-in) метрика оценки для использования.\n",
    "    - Если список `list of str`, должен быть список нескольких встроенных метрик оценки для использования.\n",
    "    - Если вызываемая (`callable`),то пользовательская метрика оценки. Сигнатура вызова - `func(y_predicted, y_true)`, где `y_true` будет объектом `DMatrix`, может потребоваться вызвать метод `get_label`. Он должен возвращать пару `str, value`, где `str` - это имя для оценки, а `value` - это значение функции оценки. Вызываемая пользовательская задача (objective) всегда сводится к минимуму.\n",
    "    \n",
    "- **`early_stopping_rounds (int)`** \n",
    "    - Активирует раннюю остановку.\n",
    "    - Чтобы продолжить обучение, показатель валидации должен улучшаться хотя бы раз в каждом раунде `early_stopping_rounds`.\n",
    "    - Требуется хотя бы один элемент в `eval_set`.\n",
    "    - Метод возвращает модель с последней итерации (а не самой лучшей).\n",
    "    - Если в `eval_set` более одного элемента, для ранней остановки будет использоваться *последняя запись* (last entry).\n",
    "    - Если в `eval_metric` более одной метрики, для ранней остановки будет использоваться *последняя метрика* (last metric).\n",
    "    - Если произойдет ранняя остановка, модель будет иметь три дополнительных поля: `clf.best_score`, `clf.best_iteration` и `clf.best_ntree_limit`.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Глобальная конфигурация (**Global Configuration**)\n",
    "- Параметр **`verbosity`** может быть установлен в глобальной области с помощью `xgb.config_context()`\n",
    "\n",
    "    - Детальность печатных сообщений.\n",
    "\n",
    "    - Допустимые значения:\n",
    "        - 0 (silent)\n",
    "        - 1 (warning)\n",
    "        - 2 (info)\n",
    "        - 3 (debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Общие параметры (**General parameters**)\n",
    "относятся к тому, какой booster используется для boosting, обычно это древовидная (tree) или линейная (linear) модель.\n",
    "\n",
    "\n",
    "- `booster [default= gbtree]`\n",
    "    - Какой booster использовать.\n",
    "    - Допустимые значения:\n",
    "        - `gbtree`\n",
    "        - `gblinear`\n",
    "        - `dart`\n",
    "    - `gbtree` и `dart` используют модели на основе дерева (tree based models), в то время как `gblinear` использует линейные функции (linear functions).\n",
    "\n",
    "\n",
    "- `verbosity [default=1]`\n",
    "    - Детальность печатных сообщений.\n",
    "    - Допустимые значения:\n",
    "        - 0 (silent)\n",
    "        - 1 (warning)\n",
    "        - 2 (info)\n",
    "        - 3 (debug)\n",
    "    - Иногда XGBoost пытается изменить конфигурации на основе эвристики, что отображается как предупреждающее сообщение. Если происходит неожиданное поведение, попробуйте увеличить значение подробности.\n",
    "    - *Эвристический алгоритм (эвристика) — алгоритм решения задачи, включающий практический метод, не являющийся гарантированно точным или оптимальным, но достаточный для решения поставленной задачи. Позволяет ускорить решение задачи в тех случаях, когда точное решение не может быть найдено.\n",
    "\n",
    "\n",
    "- `validate_parameters [default=True]`\n",
    "    - Если установлено значение `True`, XGBoost будет выполнять проверку входных параметров, чтобы проверить, используется ли параметр или нет.\n",
    "    - *Эта функция все еще экспериментальная. Ожидается несколько ложных срабатываний.*\n",
    "\n",
    "\n",
    "- `nthread [default=максимальное количество доступных потоков, если не установлено]`\n",
    "    - Количество параллельных потоков, используемых для запуска XGBoost.\n",
    "    - *При выборе учитывайте конфликты потоков и гиперпоточность.*\n",
    "\n",
    "\n",
    "- `disable_default_eval_metric [default='false']`\n",
    "    - Используйте `'true'` (или `1`), чтобы отключить метрику по умолчанию.\n",
    "\n",
    "\n",
    "- `num_pbuffer [устанавливается автоматически XGBoost, нет необходимости устанавливать пользователем]`\n",
    "    - Размер буфера предсказания, обычно равный количеству обучающих экземпляров (training instances). Буферы используются для сохранения результатов прогноза последнего boosting шага.\n",
    "\n",
    "\n",
    "- `num_feature [устанавливается автоматически XGBoost, нет необходимости устанавливать пользователем]`\n",
    "    - Размер объекта, используемый при повышении, установлен на максимальный размер объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Параметры booster'а (**Booster parameters**)\n",
    "зависят от того какой booster выбран."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "##### Параметры для Tree Booster (**Parameters for Tree Booster**)\n",
    "\n",
    "- `eta [default=0.3, alias: learning_rate]`\n",
    "    - Уменьшение размера шага, используемое в обновлении, чтобы **предотвратить переобучение (overfitting)**. После каждого шага повышения мы можем напрямую получить веса новых функций, а eta уменьшает веса функций, чтобы сделать процесс повышения более консервативным.\n",
    "    - Допустимые значения: $[0, 1]$\n",
    "\n",
    "\n",
    "- `gamma [default=0, alias: min_split_loss]`\n",
    "    - Минимальное изменение значения функции потерь (Minimum loss reduction), необходимое для дальнейшего разбиения листового узла (leaf node) дерева (tree), то есть разделение листа на поддеревья. Чем больше `gamma`, тем более \"консервативным\" будет алгоритм.\n",
    "    - Допустимые значения: $[0, \\infty]$\n",
    "\n",
    "\n",
    "- `max_depth [default=6]`\n",
    "    - Максимальная глубина дерева. Увеличение этого значения сделает модель более сложной и с большей вероятностью переобучится (overfit).\n",
    "    - Допустимые значения: $[0, \\infty]$, `0` принимается только в lossguided для роста, когда tree_method установлен как hist и указывает отсутствие ограничения на глубину. Помните, что XGBoost агрессивно расходует память при обучении глубокого дерева.\n",
    "\n",
    "\n",
    "- `min_child_weight [default=1]`\n",
    "    - Минимальная сумма веса экземпляра (hessian), необходимая для дочернего узла (child). Если на этапе разбиения дерева получается листовой узел (leaf node) с суммой веса экземпляра меньше min_child_weight, то процесс построения откажется от дальнейшего разделения. В задаче линейной регрессии (linear regression) это просто соответствует минимальному количеству экземпляров (instances), которое должно быть в каждом узле. Чем больше min_child_weight, тем более \"консервативным: будет алгоритм.\n",
    "    - Допустимые значения: $[0, \\infty]$\n",
    "\n",
    "\n",
    "- `max_delta_step [default=0]`\n",
    "    - Максимальный шаг дельты, который допускается на выходе каждого листа (leaf output).\n",
    "    - Если значение установлено на `0`, это означает, что ограничения нет.\n",
    "    - Если для него установлено `положительное значение`, это может помочь сделать шаг обновления более \"консервативным\".\n",
    "    - *Обычно этот параметр не нужен*, но он может помочь в логистической регрессии (logistic regression), когда класс крайне несбалансирован (imbalanced).\n",
    "    - Установка значения 1-10 может помочь контролировать обновление.\n",
    "    - Допустимые значения: $[0, 1]$\n",
    "\n",
    "\n",
    "- `subsample [default=1]`\n",
    "    - Соотношение подвыборки обучающих примеров. Установка его на 0.5 означает, что XGBoost будет случайным образом выбирать половину обучающих данных перед выращиванием деревьев. И это предотвратит переобучение (overfitting). Подвыборка (subsample) будет выполняться один раз на каждой итерации boosting'а.\n",
    "    - Допустимые значения: $(0, 1]$\n",
    "\n",
    "\n",
    "- `sampling_method [default= uniform]`\n",
    "    - Метод, используемый для выборки обучающих экземпляров.\n",
    "    - `uniform`: каждый обучающий экземпляр имеет равную вероятность быть выбранным. Обычно для хороших результатов задают подвыборку $\\ge$ 0.5.\n",
    "    - `gradient_based`: вероятность выбора для каждого обучающего примера пропорциональна регуляризованному абсолютному значению градиентов (более конкретно, $\\sqrt{g^{2} + \\lambda*h^{2}}$.\n",
    "    - Подвыборка (subsample) может быть установлена на уровне `0.1` без потери точности модели.\n",
    "    - Обратите внимание, что этот метод выборки поддерживается только тогда, когда `tree_method` имеет значение `gpu_hist`; другие древовидные методы поддерживают только `uniform` выборку.\n",
    "\n",
    "\n",
    "- `colsample_bytree, colsample_bylevel, colsample_bynode [default=1]`\n",
    "\n",
    "\n",
    "- `lambda [default=1, alias: reg_lambda]`\n",
    "    - Условие L2 регуляризации (ridge regression или Tikhonov regularization) весов.\n",
    "    - Увеличение этого значения сделает модель более \"консервативной\".\n",
    "\n",
    "\n",
    "- `alpha [default=0, alias: reg_alpha]`\n",
    "    - Элемент регуляризации L1 (lasso regression) на весах.\n",
    "    - Увеличение этого значения сделает модель более \"консервативной\".\n",
    "\n",
    "\n",
    "- `tree_method string [default= auto]`\n",
    "\n",
    "\n",
    "- `sketch_eps [default=0.03]`\n",
    "\n",
    "\n",
    "- `scale_pos_weight [default=1]`\n",
    "\n",
    "\n",
    "- `updater [default= grow_colmaker,prune]`\n",
    "\n",
    "\n",
    "- `refresh_leaf [default=1]`\n",
    "    - Это параметр обновления (refresh updater).\n",
    "    - Когда этот флаг равен `1`, обновляется статистика листьев (leafs) дерева и узлов (nodes) дерева.\n",
    "    - Когда он равен `0`, обновляется только статистика по узлам (nodes).\n",
    "\n",
    "\n",
    "- `process_type [default= default]`\n",
    "\n",
    "\n",
    "- `grow_policy [default= depthwise]`\n",
    "    - Управляет способом добавления новых узлов в дерево.\n",
    "    - В настоящее время поддерживается, только если для `tree_method` установлено значение `hist`.\n",
    "    - Допустимые значения: `depthwise`, `lossguide`\n",
    "        - `depthwise`: разбивать в узлах, ближайших к корню.\n",
    "        - `lossguide`: разделить в узлах с наибольшим изменением потерь (loss change).\n",
    "\n",
    "\n",
    "- `max_leaves [default=0]`\n",
    "    - Максимальное количество добавляемых узлов (nodes). Актуально, только если установлено `grow_policy = lossguide`.\n",
    "\n",
    "\n",
    "- `max_bin, [default=256]`\n",
    "    - Используется, только если `tree_method` имеет значение `hist`.\n",
    "    - Максимальное количество отдельных bin'ов для непрерывных bucket параметров (features).\n",
    "    - Увеличение этого числа улучшает оптимальность разбиения за счет увеличения времени вычислений.\n",
    "\n",
    "\n",
    "- `predictor, [default='auto']`\n",
    "\n",
    "\n",
    "- `num_parallel_tree, [default=1]`\n",
    "    - Количество параллельных деревьев (trees), построенных на каждой итерации. Эта опция используется для поддержки boost'инга случайного леса (random forest).\n",
    "\n",
    "\n",
    "- `monotone_constraints`\n",
    "    - Ограничение переменной монотонности.\n",
    "\n",
    "\n",
    "- `interaction_constraints`\n",
    "    - Ограничения для взаимодействия, представляющие разрешенные взаимодействия.\n",
    "    - Ограничения должны быть указаны в виде списка вложений, например `[[0, 1], [2, 3, 4]]`, где каждый внутренний список представляет собой группу индексов функций, которым разрешено взаимодействовать друг с другом.\n",
    "\n",
    "\n",
    "- *Дополнительный параметр для метода дерева `hist` и `gpu_hist`*\n",
    "    - `single_precision_histogram, [default='false']`\n",
    "        - Использовать одинарную точность для построения гистограмм вместо двойной точности.\n",
    "\n",
    "\n",
    "- *Дополнительный параметр для метода дерева `gpu_hist`*\n",
    "    - `deterministic_histogram, [default='true']`\n",
    "        - Построение гистограммы на GPU детерминировано.\n",
    "        - Построение гистограммы не является детерминированным из-за неассоциативного аспекта суммирования с плавающей запятой.\n",
    "        - Используется процедура предварительного округления, чтобы устранить проблему, которая может немного снизить точность.\n",
    "        - Установите значение `false`, чтобы отключить его.\n",
    "\n",
    "\n",
    "- *Дополнительные параметры для `Dart Booster` (`booster=dart`)*\n",
    "\n",
    "    - `sample_type [default= uniform]`\n",
    "\n",
    "\n",
    "    - `normalize_type [default= tree]`\n",
    "\n",
    "\n",
    "    - `rate_drop [default=0.0]`\n",
    "\n",
    "\n",
    "    - `one_drop [default=0]`\n",
    "\n",
    "\n",
    "    - `skip_drop [default=0.0]`\n",
    "\n",
    "\n",
    "- *Параметры для `Linear Booster` (`booster=gblinear`)*\n",
    "\n",
    "    - `lambda [default=0, alias: reg_lambda]`\n",
    "\n",
    "\n",
    "    - `alpha [default=0, alias: reg_alpha]`\n",
    "\n",
    "\n",
    "    - `updater [default= shotgun]`\n",
    "\n",
    "\n",
    "    - `feature_selector [default= cyclic]`\n",
    "\n",
    "\n",
    "    - `top_k [default=0]`\n",
    "\n",
    "\n",
    "- *Параметры для `Tweedie Booster` (`objective=reg:tweedie`)*\n",
    "\n",
    "    - `tweedie_variance_power [default=1.5]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Параметры учебной задачи (**Learning task parameters**)\n",
    "определяют сценарий обучения. Например, задачи регрессии (regression) могут использовать разные параметры с задачами ранжирования (ranking).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Параметры командной строки (**Command line parameters*)\n",
    "относятся к поведению CLI-версии XGBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LevelUp_DataScience] *",
   "language": "python",
   "name": "conda-env-LevelUp_DataScience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
