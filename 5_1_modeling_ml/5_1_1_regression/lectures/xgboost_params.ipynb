{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Параметры XGBRegressor (XGBRegressor Parameters)\n",
    "\n",
    "Интерфейс обертки Scikit-Learn для XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_estimators (int)` - Количество деревьев с градиентным усилением. Эквивалентно количеству раундов boost-инга (boosting rounds).\n",
    "\n",
    "\n",
    "- `max_depth (int) [default=6]`\n",
    "    - Максимальная глубина дерева. Увеличение этого значения сделает модель более сложной и с большей вероятностью переобучится (overfit).\n",
    "    - Допустимые значения: $[0, \\infty]$, `0` принимается только в lossguided для роста, когда tree_method установлен как hist и указывает отсутствие ограничения на глубину. Помните, что XGBoost агрессивно расходует память при обучении глубокого дерева.\n",
    "\n",
    "\n",
    "- `learning_rate (float) [default=0.3, alias: eta]`\n",
    "    - Уменьшение размера шага, используемое в обновлении, чтобы **предотвратить переобучение (overfitting)**. После каждого шага повышения мы можем напрямую получить веса новых функций, а eta уменьшает веса функций, чтобы сделать процесс повышения более консервативным.\n",
    "    - Допустимые значения: $[0, 1]$\n",
    "\n",
    "\n",
    "- `verbosity  (int) [default=1]`\n",
    "    - Детальность печатных сообщений.\n",
    "    - Допустимые значения:\n",
    "        - 0 (silent)\n",
    "        - 1 (warning)\n",
    "        - 2 (info)\n",
    "        - 3 (debug)\n",
    "    - Иногда XGBoost пытается изменить конфигурации на основе эвристики, что отображается как предупреждающее сообщение. Если происходит неожиданное поведение, попробуйте увеличить значение подробности.\n",
    "    - *Эвристический алгоритм (эвристика) — алгоритм решения задачи, включающий практический метод, не являющийся гарантированно точным или оптимальным, но достаточный для решения поставленной задачи. Позволяет ускорить решение задачи в тех случаях, когда точное решение не может быть найдено.\n",
    "\n",
    "\n",
    "- `objective (string | callable)` - Устанавливает задачу обучения (learning task) и соответствующую цель обучения (learning objective) или настраиваемую целевую функцию (objective function), которая будет использоваться.\n",
    "\n",
    "\n",
    "- `booster (string) [default='gbtree']`\n",
    "    - Какой booster использовать.\n",
    "    - Допустимые значения:\n",
    "        - `gbtree`\n",
    "        - `gblinear`\n",
    "        - `dart`\n",
    "    - `gbtree` и `dart` используют модели на основе дерева (tree based models), в то время как `gblinear` использует линейные функции (linear functions).\n",
    "\n",
    "\n",
    "- `tree_method (string) [default='auto']`\n",
    "    - Алгоритм построения дерева, используемый в XGBoost.\n",
    "    - XGBoost поддерживает `approx`, `hist` и `gpu_hist` для распределенного обучения. Экспериментальная поддержка внешней памяти доступна для `approx` и `gpu_hist`.\n",
    "    - Допустимые значения: `auto`, `exact`, `approx`, `hist`, `gpu_hist`, это комбинация часто используемых средств обновления. Для других средств обновления, таких как `refresh`, установите параметр `updater` напрямую.\n",
    "        - `auto`: Использует эвристику, чтобы выбрать самый быстрый метод.\n",
    "            - Для большего набора данных будет выбран `approx`. Рекомендуется попробовать `hist` и `gpu_hist` для повышения производительности с большим набором данных. `gpu_hist` поддерживает внешнюю память.\n",
    "            - Поскольку старое поведение всегда использует точную жадность ([exact greedy](https://medium.com/analytics-vidhya/boosting-models-unwrapping-the-basic-exact-greedy-algorithm-b67d88c7189a)) на одной машине, пользователь получит сообщение, когда будет выбран `approx` алгоритм для уведомления об этом выборе.\n",
    "        - `exact`: Точный жадный алгоритм ([exact greedy](https://medium.com/analytics-vidhya/boosting-models-unwrapping-the-basic-exact-greedy-algorithm-b67d88c7189a)). Перечисляет всех отдельных кандидатов.\n",
    "        - `approx`: Приблизительный жадный алгоритм ([approximate greedy](https://towardsdatascience.com/why-xgboost-is-so-effective-3a193951e289)) с использованием квантилей ([quantile sketch](https://datasketches.apache.org/docs/Quantiles/QuantilesOverview.html)) и гистограммы градиента.\n",
    "        - `hist`: Более быстрый приближенный жадный алгоритм Приблизительный жадный алгоритм ([approximate greedy](https://towardsdatascience.com/why-xgboost-is-so-effective-3a193951e289)), оптимизированный для гистограммы.\n",
    "        - `gpu_hist`: Реализация алгоритма `hist` на GPU.\n",
    "\n",
    "- n_jobs (int) - Количество параллельных потоков, используемых для запуска `xgboost`. При использовании с другими алгоритмами Scikit-Learn, такими как `grid search`, можно выбрать, какой алгоритм распараллеливать и балансировать потоки. Создание конкуренции потоков ([thread contention](https://stackoverflow.com/questions/1970345/what-is-thread-contention#:~:text=Essentially%20thread%20contention%20is%20a,has%20unlocked%20that%20particular%20object.)) значительно замедлит работу обоих алгоритмов.\n",
    "\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Глобальная конфигурация (**Global Configuration**)\n",
    "- Параметр **`verbosity`** может быть установлен в глобальной области с помощью `xgb.config_context()`\n",
    "\n",
    "    - Детальность печатных сообщений.\n",
    "\n",
    "    - Допустимые значения:\n",
    "        - 0 (silent)\n",
    "        - 1 (warning)\n",
    "        - 2 (info)\n",
    "        - 3 (debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Общие параметры (**General parameters**)\n",
    "относятся к тому, какой booster используется для boosting, обычно это древовидная (tree) или линейная (linear) модель.\n",
    "\n",
    "\n",
    "- `booster [default= gbtree]`\n",
    "    - Какой booster использовать.\n",
    "    - Допустимые значения:\n",
    "        - `gbtree`\n",
    "        - `gblinear`\n",
    "        - `dart`\n",
    "    - `gbtree` и `dart` используют модели на основе дерева (tree based models), в то время как `gblinear` использует линейные функции (linear functions).\n",
    "\n",
    "\n",
    "- `verbosity [default=1]`\n",
    "    - Детальность печатных сообщений.\n",
    "    - Допустимые значения:\n",
    "        - 0 (silent)\n",
    "        - 1 (warning)\n",
    "        - 2 (info)\n",
    "        - 3 (debug)\n",
    "    - Иногда XGBoost пытается изменить конфигурации на основе эвристики, что отображается как предупреждающее сообщение. Если происходит неожиданное поведение, попробуйте увеличить значение подробности.\n",
    "    - *Эвристический алгоритм (эвристика) — алгоритм решения задачи, включающий практический метод, не являющийся гарантированно точным или оптимальным, но достаточный для решения поставленной задачи. Позволяет ускорить решение задачи в тех случаях, когда точное решение не может быть найдено.\n",
    "\n",
    "\n",
    "- `validate_parameters [default=True]`\n",
    "    - Если установлено значение `True`, XGBoost будет выполнять проверку входных параметров, чтобы проверить, используется ли параметр или нет.\n",
    "    - *Эта функция все еще экспериментальная. Ожидается несколько ложных срабатываний.*\n",
    "\n",
    "\n",
    "- `nthread [default=максимальное количество доступных потоков, если не установлено]`\n",
    "    - Количество параллельных потоков, используемых для запуска XGBoost.\n",
    "    - *При выборе учитывайте конфликты потоков и гиперпоточность.*\n",
    "\n",
    "\n",
    "- `disable_default_eval_metric [default='false']`\n",
    "    - Используйте `'true'` (или `1`), чтобы отключить метрику по умолчанию.\n",
    "\n",
    "\n",
    "- `num_pbuffer [устанавливается автоматически XGBoost, нет необходимости устанавливать пользователем]`\n",
    "    - Размер буфера предсказания, обычно равный количеству обучающих экземпляров (training instances). Буферы используются для сохранения результатов прогноза последнего boosting шага.\n",
    "\n",
    "\n",
    "- `num_feature [устанавливается автоматически XGBoost, нет необходимости устанавливать пользователем]`\n",
    "    - Размер объекта, используемый при повышении, установлен на максимальный размер объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Параметры booster'а (**Booster parameters**)\n",
    "зависят от того какой booster выбран."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Параметры для Tree Booster (**Parameters for Tree Booster**)\n",
    "\n",
    "- `eta [default=0.3, alias: learning_rate]`\n",
    "    - Уменьшение размера шага, используемое в обновлении, чтобы **предотвратить переобучение (overfitting)**. После каждого шага повышения мы можем напрямую получить веса новых функций, а eta уменьшает веса функций, чтобы сделать процесс повышения более консервативным.\n",
    "    - Допустимые значения: $[0, 1]$\n",
    "\n",
    "\n",
    "- `gamma [default=0, alias: min_split_loss]`\n",
    "    - Минимальное изменение значения функции потерь (Minimum loss reduction), необходимое для дальнейшего разбиения листового узла (leaf node) дерева (tree), то есть разделение листа на поддеревья. Чем больше `gamma`, тем более \"консервативным\" будет алгоритм.\n",
    "    - Допустимые значения: $[0, \\infty]$\n",
    "\n",
    "\n",
    "- `max_depth [default=6]`\n",
    "    - Максимальная глубина дерева. Увеличение этого значения сделает модель более сложной и с большей вероятностью переобучится (overfit).\n",
    "    - Допустимые значения: $[0, \\infty]$, `0` принимается только в lossguided для роста, когда tree_method установлен как hist и указывает отсутствие ограничения на глубину. Помните, что XGBoost агрессивно расходует память при обучении глубокого дерева.\n",
    "\n",
    "\n",
    "- `min_child_weight [default=1]`\n",
    "    - Минимальная сумма веса экземпляра (hessian), необходимая для дочернего узла (child). Если на этапе разбиения дерева получается листовой узел (leaf node) с суммой веса экземпляра меньше min_child_weight, то процесс построения откажется от дальнейшего разделения. В задаче линейной регрессии (linear regression) это просто соответствует минимальному количеству экземпляров (instances), которое должно быть в каждом узле. Чем больше min_child_weight, тем более \"консервативным: будет алгоритм.\n",
    "    - Допустимые значения: $[0, \\infty]$\n",
    "\n",
    "\n",
    "- `max_delta_step [default=0]`\n",
    "    - Максимальный шаг дельты, который допускается на выходе каждого листа (leaf output).\n",
    "    - Если значение установлено на `0`, это означает, что ограничения нет.\n",
    "    - Если для него установлено `положительное значение`, это может помочь сделать шаг обновления более \"консервативным\".\n",
    "    - *Обычно этот параметр не нужен*, но он может помочь в логистической регрессии (logistic regression), когда класс крайне несбалансирован (imbalanced).\n",
    "    - Установка значения 1-10 может помочь контролировать обновление.\n",
    "    - Допустимые значения: $[0, 1]$\n",
    "\n",
    "\n",
    "- `subsample [default=1]`\n",
    "    - Соотношение подвыборки обучающих примеров. Установка его на 0.5 означает, что XGBoost будет случайным образом выбирать половину обучающих данных перед выращиванием деревьев. И это предотвратит переобучение (overfitting). Подвыборка (subsample) будет выполняться один раз на каждой итерации boosting'а.\n",
    "    - Допустимые значения: $(0, 1]$\n",
    "\n",
    "\n",
    "- `sampling_method [default= uniform]`\n",
    "    - Метод, используемый для выборки обучающих экземпляров.\n",
    "    - `uniform`: каждый обучающий экземпляр имеет равную вероятность быть выбранным. Обычно для хороших результатов задают подвыборку $\\ge$ 0.5.\n",
    "    - `gradient_based`: вероятность выбора для каждого обучающего примера пропорциональна регуляризованному абсолютному значению градиентов (более конкретно, $\\sqrt{g^{2} + \\lambda*h^{2}}$.\n",
    "    - Подвыборка (subsample) может быть установлена на уровне `0.1` без потери точности модели.\n",
    "    - Обратите внимание, что этот метод выборки поддерживается только тогда, когда `tree_method` имеет значение `gpu_hist`; другие древовидные методы поддерживают только `uniform` выборку.\n",
    "\n",
    "\n",
    "- `colsample_bytree, colsample_bylevel, colsample_bynode [default=1]`\n",
    "\n",
    "\n",
    "- `lambda [default=1, alias: reg_lambda]`\n",
    "    - Условие L2 регуляризации (ridge regression или Tikhonov regularization) весов.\n",
    "    - Увеличение этого значения сделает модель более \"консервативной\".\n",
    "\n",
    "\n",
    "- `alpha [default=0, alias: reg_alpha]`\n",
    "    - Элемент регуляризации L1 (lasso regression) на весах.\n",
    "    - Увеличение этого значения сделает модель более \"консервативной\".\n",
    "\n",
    "\n",
    "- `tree_method string [default= auto]`\n",
    "\n",
    "\n",
    "- `sketch_eps [default=0.03]`\n",
    "\n",
    "\n",
    "- `scale_pos_weight [default=1]`\n",
    "\n",
    "\n",
    "- `updater [default= grow_colmaker,prune]`\n",
    "\n",
    "\n",
    "- `refresh_leaf [default=1]`\n",
    "    - Это параметр обновления (refresh updater).\n",
    "    - Когда этот флаг равен `1`, обновляется статистика листьев (leafs) дерева и узлов (nodes) дерева.\n",
    "    - Когда он равен `0`, обновляется только статистика по узлам (nodes).\n",
    "\n",
    "\n",
    "- `process_type [default= default]`\n",
    "\n",
    "\n",
    "- `grow_policy [default= depthwise]`\n",
    "    - Управляет способом добавления новых узлов в дерево.\n",
    "    - В настоящее время поддерживается, только если для `tree_method` установлено значение `hist`.\n",
    "    - Допустимые значения: `depthwise`, `lossguide`\n",
    "        - `depthwise`: разбивать в узлах, ближайших к корню.\n",
    "        - `lossguide`: разделить в узлах с наибольшим изменением потерь (loss change).\n",
    "\n",
    "\n",
    "- `max_leaves [default=0]`\n",
    "    - Максимальное количество добавляемых узлов (nodes). Актуально, только если установлено `grow_policy = lossguide`.\n",
    "\n",
    "\n",
    "- `max_bin, [default=256]`\n",
    "    - Используется, только если `tree_method` имеет значение `hist`.\n",
    "    - Максимальное количество отдельных bin'ов для непрерывных bucket параметров (features).\n",
    "    - Увеличение этого числа улучшает оптимальность разбиения за счет увеличения времени вычислений.\n",
    "\n",
    "\n",
    "- `predictor, [default='auto']`\n",
    "\n",
    "\n",
    "- `num_parallel_tree, [default=1]`\n",
    "    - Количество параллельных деревьев (trees), построенных на каждой итерации. Эта опция используется для поддержки boost'инга случайного леса (random forest).\n",
    "\n",
    "\n",
    "- `monotone_constraints`\n",
    "    - Ограничение переменной монотонности.\n",
    "\n",
    "\n",
    "- `interaction_constraints`\n",
    "    - Ограничения для взаимодействия, представляющие разрешенные взаимодействия.\n",
    "    - Ограничения должны быть указаны в виде списка вложений, например `[[0, 1], [2, 3, 4]]`, где каждый внутренний список представляет собой группу индексов функций, которым разрешено взаимодействовать друг с другом.\n",
    "\n",
    "\n",
    "- *Дополнительный параметр для метода дерева `hist` и `gpu_hist`*\n",
    "    - `single_precision_histogram, [default='false']`\n",
    "        - Использовать одинарную точность для построения гистограмм вместо двойной точности.\n",
    "\n",
    "\n",
    "- *Дополнительный параметр для метода дерева `gpu_hist`*\n",
    "    - `deterministic_histogram, [default='true']`\n",
    "        - Построение гистограммы на GPU детерминировано.\n",
    "        - Построение гистограммы не является детерминированным из-за неассоциативного аспекта суммирования с плавающей запятой.\n",
    "        - Используется процедура предварительного округления, чтобы устранить проблему, которая может немного снизить точность.\n",
    "        - Установите значение `false`, чтобы отключить его.\n",
    "\n",
    "\n",
    "- *Дополнительные параметры для `Dart Booster` (`booster=dart`)*\n",
    "\n",
    "    - `sample_type [default= uniform]`\n",
    "\n",
    "\n",
    "    - `normalize_type [default= tree]`\n",
    "\n",
    "\n",
    "    - `rate_drop [default=0.0]`\n",
    "\n",
    "\n",
    "    - `one_drop [default=0]`\n",
    "\n",
    "\n",
    "    - `skip_drop [default=0.0]`\n",
    "\n",
    "\n",
    "- *Параметры для `Linear Booster` (`booster=gblinear`)*\n",
    "\n",
    "    - `lambda [default=0, alias: reg_lambda]`\n",
    "\n",
    "\n",
    "    - `alpha [default=0, alias: reg_alpha]`\n",
    "\n",
    "\n",
    "    - `updater [default= shotgun]`\n",
    "\n",
    "\n",
    "    - `feature_selector [default= cyclic]`\n",
    "\n",
    "\n",
    "    - `top_k [default=0]`\n",
    "\n",
    "\n",
    "- *Параметры для `Tweedie Booster` (`objective=reg:tweedie`)*\n",
    "\n",
    "    - `tweedie_variance_power [default=1.5]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Параметры учебной задачи (**Learning task parameters**)\n",
    "определяют сценарий обучения. Например, задачи регрессии (regression) могут использовать разные параметры с задачами ранжирования (ranking).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Параметры командной строки (**Command line parameters*)\n",
    "относятся к поведению CLI-версии XGBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LevelUp_DataScience] *",
   "language": "python",
   "name": "conda-env-LevelUp_DataScience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
