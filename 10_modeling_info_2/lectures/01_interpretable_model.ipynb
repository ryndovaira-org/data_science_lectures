{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Интерпретируемость модели / Explainable artificial intelligence (XAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "**Источники:**\n",
    "\n",
    "[Интерпретируемые модели](http://neerc.ifmo.ru/wiki/index.php?title=%D0%98%D0%BD%D1%82%D0%B5%D1%80%D0%BF%D1%80%D0%B5%D1%82%D0%B8%D1%80%D1%83%D0%B5%D0%BC%D1%8B%D0%B5_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8)\n",
    "\n",
    "[Interpretable Machine Learning](https://towardsdatascience.com/interpretable-machine-learning-45b467dbe1af)\n",
    "\n",
    "[Explainable Reinforcement Learning: A Survey](https://arxiv.org/pdf/2005.06247.pdf)\n",
    "\n",
    "[Analysis of Explainers of Black Box Deep Neural\n",
    "Networks for Computer Vision: A Survey](http://neerc.ifmo.ru/wiki/index.php?title=%D0%98%D0%BD%D1%82%D0%B5%D1%80%D0%BF%D1%80%D0%B5%D1%82%D0%B8%D1%80%D1%83%D0%B5%D0%BC%D1%8B%D0%B5_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8)\n",
    "\n",
    "[Approximate and Situated Causality in Deep Learning](https://www.researchgate.net/publication/339079867_Approximate_and_Situated_Causality_in_Deep_Learning)\n",
    "\n",
    "[Counterfactuals and Causability in Explainable Artificial Intelligence: Theory, Algorithms, and Applications](https://www.researchgate.net/publication/349914003_Counterfactuals_and_Causability_in_Explainable_Artificial_Intelligence_Theory_Algorithms_and_Applications)\n",
    "\n",
    "[SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap)\n",
    "\n",
    "[LIME (англ. Local Interpretable Model-agnostic Explanations)](https://github.com/marcotcr/lime)\n",
    "\n",
    "[Explainable artificial intelligence](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence)\n",
    "\n",
    "[A taxonomy of explainable (XAI) AI models](https://www.datasciencecentral.com/profiles/blogs/a-taxonomy-of-explainable-xai-ai-models)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Интерпретируемость\n",
    "\n",
    "**Интерпретируемость** — это свойство модели, которое показывает, что **структуру данной модели может объяснить человек**. \n",
    "\n",
    "При этом структура модели не противоречит данным, на которых данная модель построена, а также она **сохраняет некоторые свойства предоставленных данных**. \n",
    "\n",
    "При интерпретации модели могут быть **объяснены принципы и закономерности, которые использует сама модель для предсказания меток класса на конкретных данных**.\n",
    "\n",
    "Если модель машинного обучения работает хорошо, почему просто не доверять модели и игнорировать факторы, из-за которых модель приняла решение? Проблема в том, что **используя только метрику для измерения точности предсказания, возможно такое, что задача будет решена не полностью или даже неправильно**. Часто важны причины, по которым модель сделала то или иное предсказание.\n",
    "\n",
    "Пример: модель решает, когда нужно класть ковидного больного в палату, а когда отправлять лечиться дома. По статистике люди болеющие астмой выживают чаще, чем здоровые, и логично предположить, что их можно отправлять лечится дома, но дело в том, что этих людей врачи лечат тщательней, поэтому они и выживают чаще. Если бы мы верили модели в слепую, то люди с астмой просто бы умирали. Поэтому нам важно понять, почему модель пришла к тому или иному выводу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В каких случаях интерпретируемость нужна?\n",
    "\n",
    "- Когда целью является **получение каких-либо знаний с помощью изучения построенной модели**.\n",
    "\n",
    "- Когда алгоритм оптимизировал **неполную цель**. Например, когда автомобильный инженер получает предсказания о параметрах двигателя, но ему нужно построить в целом достаточно хороший и надёжный автомобиль.\n",
    "\n",
    "- Для **безопасности сложных систем**. Такие системы, в большинстве случаев, нельзя протестировать от начала до конца. Вычислительно тяжело просмотреть все возможное входные данные и сценарии развития событий.\n",
    "\n",
    "- **Интерпретация тренировочных данных**, поиск выбросов в них.\n",
    "\n",
    "- **Поиск ошибок** в выводе модели.\n",
    "\n",
    "- Пользователь может больше **доверять** модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В каких случаях интерпретируемость НЕ нужна?\n",
    "\n",
    "- **Влияние модели мало (= небольшие потери при ошибке)**, а сама интерпретация требует большого количества ресурсов (предложение новых покупок на основе предыдущих).\n",
    "\n",
    "- **Проблема хорошо разработана**, и специалистов обучают ещё в университетах.\n",
    "\n",
    "- Выбранный **тип модели легко интерпретируется** без дополнительных средств: линейные модели (стоимость квартиры: понятно, что это метраж, расстояние до метро, школы, детского сада и т.д., но когда параметров много, то уже сложно всё это держать в голове).\n",
    "\n",
    "- Необходимость **скрыть систему** (кому давать кредит, качество работы сотрудника, поисковое ранжирование)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Способы создания интерпретируемой модели\n",
    "\n",
    "- Использовать только интерпретируемые модели. НО: не всё хорошо описывается этими моделями.\n",
    "    - Линейные модели. \n",
    "    - Деревья решений.\n",
    "    - Правила.\n",
    "    - <img src=\"images/interpretable_models.jpg\" width=500>\n",
    "    \n",
    "\n",
    "- Построить интерпретируемую **модель поверх** (эмбенддинга). НО: модель теперь интерпретируема, а сами признаки перестают быть таковым.\n",
    "    - Эмбеддинг:\n",
    "        - В NLP - это векторное (список действительных чисел) представление слова aka  Word Embeddings.\n",
    "        - Сам по себе **embedding — это сопоставление произвольной сущности (например, узла в графе или кусочка картинки, или слова или ...) некоторому вектору с числами**.\n",
    "    - Пример: Есть лук. Если \"лук\" находится рядом с \"чесноком\", то модель думает о \"луке\" как об овоще, если \"лук\" находится рядом с \"пистолетом\", \"рогаткой\", то модель думает о \"луке\" как об оружии.\n",
    "\n",
    "\n",
    "- Важность признаков. Одна из возможностей проанализировать модель — **оценить, насколько её решение зависит от отдельных признаков**, какой признак внёс наибольший вес для нахождения решения модели.\n",
    "    - Пример:   Модель определяет кто на картинке собака или волк. Допустим выборка для обучения оказалось не самой удачной, и все картинки с волками были на снегу, а с собаками на асфальте. Соответственно модель могла начать определять собаку или волка по асфальту или снегу. \n",
    "    - Благодаря данному виду интерпретации, модель нам можешь сказать, что главным признаком для принятия решения было не само животное, а её окружение.\n",
    "    - Данную идею реализуют с помощью значений Шепли.\n",
    "    - [Значения/Вектор Шепли (англ. Shapley values)](https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80_%D0%A8%D0%B5%D0%BF%D0%BB%D0%B8) — метод из коалиционной теории игр, который помогает определить, как наиболее честно распределить выигрыш между игроками в зависимости от их вклада в победу. Игроки объединяются в коалиции, чтобы получить некоторую выгоду от этого объединения. В машинном обучении в качестве игроков выступают признаки, а в качестве выигрыша — вклад в предсказание. **Подходит для задач классификации и регрессии.**\n",
    "    - Из вклада коалиций рассчитывается вклад каждого признака в итоговый прогноз. Значение Шепли — среднее между маргинальными вкладами всех возможных коалиций.\n",
    "    На основе значений Шепли Люндебергом и Ли предложен **метод [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap), объясняющий индивидуальные предсказания. Доступна его реализация на Python**.\n",
    "    - <img src=\"images/shap_header.png\" width=600>\n",
    "    - <img src=\"images/shap_diagram.png\" width=500>\n",
    "\n",
    "\n",
    "- Суррогатные модели. Для интерпретации модели (далее именуемой как черный ящик) можно использовать интерпретируемую суррогатную **модель, обученную на выводе черного ящика при различных входных данных**. \n",
    "    - Так как суррогатная модель будет повторять поведение черного ящика, то на её основе можно интерпретировать данный черный ящик.\n",
    "    - Есть два типа суррогатных моделей: **глобальная** и **локальная**.\n",
    "    - **Глобальная** суррогатная модель обучена на **всем выводе** черного ящика. Такая модель **полностью повторяет поведение** черного ящика, соответственно интерпретирует его на всей выборке.\n",
    "    - Локальная суррогатная модель** обучена на выводе в какой-то окрестности определенной точки**. Такая модель зачастую плохо интерпретирует всю выборку, но хорошо справляется с этой задачей в данной окрестности.\n",
    "    - **Глобальную** суррогатную модель довольно **сложно** построить, поэтому **чаще всего прибегают к локальным** моделям и интерпретируют определенные объекты.\n",
    "    - [LIME (англ. Local Interpretable Model-agnostic Explanations)](https://github.com/marcotcr/lime) — это библиотека, которая строит локальную суррогатную модель.\n",
    "    - <img src=\"images/lime.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Классификация моделей\n",
    "\n",
    "<img src=\"images/pseudo_ontology_XAI_methods_taxonomy.png\" width=400>\n",
    "\n",
    "- Post-Hoc (воспринимает модель как \"черный ящик\", например, нейросеть) vs Intrinsic (накладывают ограничения на сложность самой модели).\n",
    "\n",
    "- Специфические (работают только для конкретной архитектуры модели) vs агностические (можно применить ко всем моделям для решения конкретной задачи).\n",
    "\n",
    "- Локальные (позволяют понять предсказание для конкретного объекта) vs глобальные (понимание в целом, какие признаки влияют на предсказание).\n",
    "\n",
    "<img src=\"images/explainability_ml_dl.png\">\n",
    "\n",
    "<img src=\"images/taxonomy_explainable_ai.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LevelUp_DataScience] *",
   "language": "python",
   "name": "conda-env-LevelUp_DataScience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
