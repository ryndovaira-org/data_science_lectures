{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Переобучение (Overfitting) & Недообучение (Underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Источники:**\n",
    "\n",
    "[Переобучение](https://neerc.ifmo.ru/wiki/index.php?title=%D0%9F%D0%B5%D1%80%D0%B5%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5)\n",
    "\n",
    "[Переобучение](https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D0%B5%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5)\n",
    "\n",
    "[Underfitting Vs Just right Vs Overfitting in Machine learning](https://www.kaggle.com/getting-started/166897)\n",
    "\n",
    "[Underfitting vs. Overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html)\n",
    "\n",
    "[Bias–variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n",
    "\n",
    "[Дилемма смещения–дисперсии](https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D0%BB%D0%B5%D0%BC%D0%BC%D0%B0_%D1%81%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D1%8F%E2%80%93%D0%B4%D0%B8%D1%81%D0%BF%D0%B5%D1%80%D1%81%D0%B8%D0%B8)\n",
    "\n",
    "[Dilution (neural networks)](https://en.wikipedia.org/wiki/Dilution_(neural_networks))\n",
    "\n",
    "[Исключение (нейронные сети)](https://ru.wikipedia.org/wiki/%D0%98%D1%81%D0%BA%D0%BB%D1%8E%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_(%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8))\n",
    "\n",
    "[Decision tree pruning](https://en.wikipedia.org/wiki/Decision_tree_pruning)\n",
    "\n",
    "[Why is Bias-Variance Tradeoff important after all?](https://data-enigma.co/es/2021/02/03/why-is-bias-variance-tradeoff-important-after-all/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переобучение (overfitting) и Недообучение (underfitting)\n",
    "\n",
    "**Переобучение (англ. overfitting)** — негативное явление, возникающее, когда алгоритм обучения вырабатывает предсказания, которые слишком близко или точно соответствуют конкретному набору данных и поэтому не подходят для применения алгоритма к дополнительным данным или будущим наблюдениям.\n",
    "\n",
    "**Недообучение (англ. underfitting)** — негативное явление, при котором алгоритм обучения не обеспечивает достаточно малой величины средней ошибки на обучающей выборке. Недообучение возникает при использовании недостаточно сложных моделей.\n",
    "\n",
    "<img src=\"images/plot_underfitting_overfitting.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кривые обучения\n",
    "\n",
    "**Кривая обучения** — графическое представление того, как изменение меры обученности (по вертикальной оси) зависит от определенной единицы измерения опыта (по горизонтальной оси). \n",
    "\n",
    "Например, в примерах ниже представлена зависимость средней ошибки от объема датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кривые обучения при переобучении\n",
    "\n",
    "При переобучении небольшая средняя ошибка на обучающей выборке не обеспечивает такую же малую ошибку на тестовой выборке.\n",
    "\n",
    "<img src=\"images/high_variance_learning_curve.png\" width=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кривые обучения при недообучении\n",
    "\n",
    "При недообучении независимо от объема обучающего датасета как на обучающей выборке, так и на тестовой выборке небольшая средняя ошибка не достигается.\n",
    "\n",
    "<img src=\"images/high_bias_learning_curve.png\" width=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High variance и high bias\n",
    "\n",
    "**Bias (Смещение)** — ошибка неверных предположений в алгоритме обучения. Высокий bias может привести к недообучению.\n",
    "\n",
    "**Variance (Дисперсия)** — ошибка, вызванная большой чувствительностью к небольшим отклонениям в тренировочном наборе. Высокая дисперсия может привести к переобучению.\n",
    "\n",
    "<img src=\"images/bias_variance.jpg\" width=400>\n",
    "\n",
    "При использовании нейронных сетей **variance** увеличивается, а **bias** уменьшается с увеличением количества скрытых слоев.\n",
    "\n",
    "Для устранения **high variance** и **high bias** можно использовать смеси и ансамбли.\n",
    "\n",
    "Например, можно составить ансамбль (boosting) из нескольких моделей с высоким bias, и получить модель с небольшим bias.\n",
    "\n",
    "В другом случае при bagging соединяются несколько моделей с низким bias, а результирующая модель позволяет уменьшить variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дилемма bias-variance / Дилемма смещения–дисперсии / Bias–variance tradeoff\n",
    "\n",
    "**Дилемма bias–variance** — конфликт в попытке одновременно минимизировать bias и variance, тогда как уменьшение одного из негативных эффектов, приводит к увеличению другого.\n",
    "\n",
    "При небольшой сложности модели наблюдаем high bias. При усложнении модели bias уменьшается, но variance увеличится, что приводит к проблеме high variance.\n",
    "\n",
    "<img src=\"images/bias_variance_tradeoff.png\" width=550> <img src=\"images/bias_variance_tradeoff_2.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возможные решения при переобучении\n",
    "\n",
    "- Увеличение количества данных в наборе.\n",
    "- Уменьшение количества признаков (features).\n",
    "- Уменьшение сложности модели (высота дерева, степень полинома, ...).\n",
    "- Добавление регуляризации / увеличение коэффициента регуляризации.\n",
    "    - L1 / L2 / Elastic Net.\n",
    "    - Ранняя остановка ([early stopping](https://en.wikipedia.org/wiki/Early_stopping))\n",
    "    - Dilution/Dropout (NNs).\n",
    "- Перекрёстная проверка (Cross-validation).\n",
    "- Decision tree pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возможные решения при недообучении\n",
    "\n",
    "- Добавление новых признаков (features).\n",
    "- Использование более сложных моделей (ансамбли, высота дерева, степень полинома, ...).\n",
    "- Уменьшение коэффициента регуляризации.\n",
    "- Перепроверить выбросы (outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fitting.png\" width=800>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}