{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function (Функция активации)\n",
    "\n",
    "- В искусственных нейронных сетях функция активации нейрона **определяет выходной сигнал**, который определяется входным сигналом или набором входных сигналов. \n",
    "- Это функция, которая добавляется в искусственную нейронную сеть, чтобы помочь сети изучить сложные закономерности в данных.\n",
    "- В искусственных нейронных сетях эта функция также называется **передаточной функцией**.\n",
    "- В биологических нейронных сетях функция активации обычно является абстракцией, представляющей **скорость возбуждения потенциала действия в клетке**. \n",
    "- Функция принимает выходной сигнал из предыдущей ячейки и преобразует его в некоторую форму, которую можно использовать в качестве входных данных для следующей ячейки.\n",
    "- Наиболее важной особенностью функции активации является ее **способность добавлять нелинейность в нейронную сеть**.\n",
    "- В отличии от биологической функции, математическая фунция активации также **помогает сохранить значение вывода нейрона ограниченным** до определенного предела в соответсвии с требованиями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Математическая модель нейрона\n",
    "\n",
    "Input into the activation function is $W \\cdot x + b$ where \n",
    "- $W$ &ndash; **weights** of the cell \n",
    "- $x$ &ndash; **inputs**\n",
    "- $b$ &ndash; **bias** \n",
    "\n",
    "Значение этого выражения, если оно не ограничено определенным пределом, может быть очень большим, особенно в случае очень глубоких нейронных сетей с миллионами параметров. Это приведет к вычислительным проблемам. Поэтому необходимо вводить ограничение на значения.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/ryndovaira-org/data_science_notes/main/diagrams/neuron_math_model.drawio.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "\"$\\doteq$\" (two lines with a dot above them) used for \"is defined as\".\n",
    "\n",
    "TODO\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/ryndovaira-org/data_science_notes/main/diagrams/activation_functions.drawio.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendations\n",
    "\n",
    "- **Start with ReLU** in your network. \n",
    "    - Activation layer is added after the weight layer (something like CNN, RNN, LSTM or linear dense layer). \n",
    "    - If you think the model has **stopped learning**, then you can replace it with a **LeakyReLU to avoid the Dying ReLU problem**. However, the Leaky ReLU will increase the computation time a little bit.\n",
    "- If you have **Batch-Norm layers** in your network, that is added before the activation function making the order CNN-Batch Norm-Act. \n",
    "    - Although the order of Batch-Norm and Activation function is a topic of debate and some say that the order doesn’t matter.\n",
    "- Activation functions work **best in their default hyperparameters that are used in popular frameworks** such as Tensorflow and Pytorch.\n",
    "    - However, one can fiddle with the negative slope in LeakyReLU and set it to 0.02 to expedite learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Желательные свойства функций активации\n",
    "\n",
    "- **Нелинейность**\n",
    "    - Если функция активации нелинейна, можно доказать, что двухуровневая нейронная сеть будет универсальным аппроксиматором функции. Тождественная функция активации не удовлетворяет этому свойству.\n",
    "    - Если несколько уровней используют тождественную функцию активации, вся сеть эквивалентна одноуровневой модели.\n",
    "- **Непрерывная дифференцируемость**\n",
    "    - Это свойство желательно (RELU не является непрерывно дифференцируемой и имеет некоторые проблемы с оптимизацией, основанной на градиентном спуске, но остаётся допустимой возможностью) для обеспечения методов оптимизации на основе градиентного спуска.\n",
    "    - Двоичная ступенчатая функция активации не дифференцируема в точке 0 и её производная равна 0 во всех других точках, так что методы градиентного спуска не дают никакого успеха для неё.\n",
    "- **Область значений**\n",
    "    - Если множество значений функции активации ограничено, методы обучения на основе градиента более стабильны, поскольку представления эталонов существенно влияют лишь на ограниченный набор весов связей.\n",
    "    - Если область значений бесконечна, обучение, как правило, более эффективно, поскольку представления эталонов существенно влияют на большинство весов. В последнем случае обычно необходим меньший темп обучения.\n",
    "- **Монотонность**\n",
    "    - Если функция активации монотонна, поверхность ошибок, ассоциированная с одноуровневой моделью, гарантированно будет выпуклой.\n",
    "- **Гладкие функции с монотонной производной**\n",
    "    - Показано, что в некоторых случаях они обеспечивают более высокую степень общности.\n",
    "- **Аппроксимирует тождественную функцию около начала координат**\n",
    "    - Если функции активации имеют это свойство, нейронная сеть будет обучаться эффективно, если её веса инициализированы малыми случайными значениями.\n",
    "    - Если функция активации не аппроксимирует тождество около начала координат, нужно быть осторожным при инициализации весов.\n",
    "- **Проблема исчезающего градиента**\n",
    "    - Нейронные сети обучаются с использованием процесса градиентного спуска. Необходимо, чтобы наша функция активации не сдвигала градиент к нулю.\n",
    "- **Zero-Centered**\n",
    "    - Выходные данные функции активации должны быть симметричны нулю, чтобы градиенты не смещались в определенном направлении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "[Activation function](https://en.wikipedia.org/wiki/Activation_function)\n",
    "\n",
    "[Функция активации](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%B0%D1%86%D0%B8%D0%B8)\n",
    "\n",
    "[Activation Functions in Neural Networks](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
    "\n",
    "[Everything you need to know about “Activation Functions” in Deep learning models](https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253)\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data-science-course] *",
   "language": "python",
   "name": "conda-env-data-science-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
