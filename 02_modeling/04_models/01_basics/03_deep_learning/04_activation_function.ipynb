{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function (Функция активации)\n",
    "\n",
    "- В искусственных нейронных сетях функция активации нейрона **определяет выходной сигнал**, который определяется входным сигналом или набором входных сигналов. \n",
    "- В искусственных нейронных сетях эта функция также называется **передаточной функцией**.\n",
    "- В биологических нейронных сетях функция активации обычно является абстракцией, представляющей **скорость возбуждения потенциала действия в клетке**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Желательные свойства функций активации\n",
    "\n",
    "- Нелинейность\n",
    "    - Если функция активации нелинейна, можно доказать, что двухуровневая нейронная сеть будет универсальным аппроксиматором функции. Тождественная функция активации не удовлетворяет этому свойству.\n",
    "    - Если несколько уровней используют тождественную функцию активации, вся сеть эквивалентна одноуровневой модели.\n",
    "- Непрерывная дифференцируемость\n",
    "    - Это свойство желательно (RELU не является непрерывно дифференцируемой и имеет некоторые проблемы с оптимизацией, основанной на градиентном спуске, но остаётся допустимой возможностью) для обеспечения методов оптимизации на основе градиентного спуска.\n",
    "    - Двоичная ступенчатая функция активации не дифференцируема в точке 0 и её производная равна 0 во всех других точках, так что методы градиентного спуска не дают никакого успеха для неё.\n",
    "- Область значений\n",
    "    - Если множество значений функции активации ограничено, методы обучения на основе градиента более стабильны, поскольку представления эталонов существенно влияют лишь на ограниченный набор весов связей.\n",
    "    - Если область значений бесконечна, обучение, как правило, более эффективно, поскольку представления эталонов существенно влияют на большинство весов. В последнем случае обычно необходим меньший темп обучения.\n",
    "- Монотонность\n",
    "    - Если функция активации монотонна, поверхность ошибок, ассоциированная с одноуровневой моделью, гарантированно будет выпуклой.\n",
    "- Гладкие функции с монотонной производной\n",
    "    - Показано, что в некоторых случаях они обеспечивают более высокую степень общности.\n",
    "- Аппроксимирует тождественную функцию около начала координат\n",
    "    - Если функции активации имеют это свойство, нейронная сеть будет обучаться эффективно, если её веса инициализированы малыми случайными значениями.\n",
    "    - Если функция активации не аппроксимирует тождество около начала координат, нужно быть осторожным при инициализации весов. В таблице ниже функции активации, у которых $f(0)=0, f'(0)= 1, f'\\text{ непрерывна в точке 0}$, помечены как имеющие это свойство."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Activation function](https://en.wikipedia.org/wiki/Activation_function)\n",
    "\n",
    "[Функция активации](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%B0%D1%86%D0%B8%D0%B8)\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
