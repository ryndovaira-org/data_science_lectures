{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отбор признаков (Feature selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\n",
    "[Отбор признаков](https://ru.wikipedia.org/wiki/%D0%9E%D1%82%D0%B1%D0%BE%D1%80_%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D0%B2)\n",
    "\n",
    "[Выделение признаков](https://ru.wikipedia.org/wiki/%D0%92%D1%8B%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D0%B2)\n",
    "\n",
    "[Feature selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "\n",
    "[Applied Predictive Modeling](https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=as_li_ss_tl?dchild=1&keywords=Applied+Predictive+Modeling&qid=1588722749&s=books&sr=1-1&linkCode=sl1&tag=inspiredalgor-20&linkId=45aa03c24c87a9e32f5611baa5e287ff&language=en_US)\n",
    "\n",
    "[How to Choose a Feature Selection Method For Machine Learning](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)\n",
    "\n",
    "[The 5 Feature Selection Algorithms every Data Scientist should know](https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2)\n",
    "\n",
    "[Feature Selection Techniques in Machine Learning](https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/)\n",
    "\n",
    "[Automated feature selection with sklearn](https://www.kaggle.com/residentmario/automated-feature-selection-with-sklearn)\n",
    "\n",
    "[A survey on feature selection methods](https://www.sciencedirect.com/science/article/abs/pii/S0045790613003066)\n",
    "\n",
    "[Basic Feature Engineering to Reach More Efficient Machine Learning](https://towardsdatascience.com/basic-feature-engineering-to-reach-more-efficient-machine-learning-6294022e17a5)\n",
    "\n",
    "[How to Perform Feature Selection for Regression Data](https://machinelearningmastery.com/feature-selection-for-regression-data/)\n",
    "\n",
    "[Selecting good features – Part III: random forests](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
    "\n",
    "\n",
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков\n",
    "\n",
    "**Отбор признаков**, также известный как **отбор переменных**, **отбор атрибутов** или **отбор поднабора переменных**, в редких случаях **генерализация** — это разновидность абстрагирования, процесс отбора подмножества значимых признаков (переменных зависимых и независимых) для использования в построении модели.\n",
    "\n",
    "Техники отбора признаков используются по четырём причинам:\n",
    "- упрощение моделей для того, чтобы сделать их проще для интерпретации исследователями/пользователями, \n",
    "- более короткое время тренировки, \n",
    "- чтобы избежать проклятия размерности,\n",
    "- улучшенное обобщение путём сокращения переобучения (формально, уменьшение дисперсии).\n",
    "\n",
    "Центральный посыл использования техники отбора признаков, что данные содержат некоторые признаки, которые либо **излишни (redundant)**, либо **незначимы (non-informative)**, а потому могут быть удалены без существенной потери информации.\n",
    "\n",
    "**\"Излишний\"** и **\"незначимый\"** являются двумя различными понятиями, поскольку **один значимый признак может быть излишним при присутствии другого существенного признака, с которым он сильно коррелирует**.\n",
    "\n",
    "Техники **отбора признаков** следует отличать от **выделения признаков (feature extraction)**. **Выделение признаков создаёт новые признаки как функции** от оригинальных признаков, в то время как **отбор признаков возвращает подмножество признаков**.\n",
    "\n",
    "**Техники отбора** признаков часто используются в областях, где имеется **много признаков и выборки сравнительно малы** (мало точек данных). Классическими местами применения отбора признаков являются анализ рукописных текстов и ДНК-микрочипы, где имеется много тысяч признаков и от десятков до сотен экземпляров выборки.\n",
    "\n",
    "\n",
    "Многие модели, особенно те, которые основаны на наклонах и пересечениях регрессии, будут оценивать параметры для каждой переменной модели. Из-за этого наличие неинформативных переменных может добавить неопределенности к прогнозам и снизить общую эффективность модели.\n",
    "\n",
    "Классы в модуле [sklearn.feature_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) могут использоваться для выбора признаков / уменьшения размерности на выборочных наборах, либо для улучшения показателей точности оценщиков (estimators), либо для повышения их производительности на очень многомерных наборах данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление признаков с низкой дисперсией (Removing features with low variance)\n",
    "\n",
    "[sklearn.feature_selection.VarianceThreshold](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) - это простой базовый подход к выбору признаков.\n",
    "\n",
    "[sklearn.feature_selection.VarianceThreshold](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) удаляет все признаки, дисперсия которых не соответствует некоторому пороговому значению. \n",
    "\n",
    "По умолчанию удаляет все признаки с нулевой дисперсией, то есть признаки, которые имеют одинаковое значение во всех выборках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Одномерный выбор признаков (Univariate feature selection)\n",
    "\n",
    "Одномерный выбор признаков работает путем выбора лучших признаков на основе одномерных статистических тестов:\n",
    "\n",
    "- Для регрессии: \n",
    "    - [`f_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "    - [`mutual_info_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html)\n",
    "    \n",
    "\n",
    "\n",
    "- Для классификации: \n",
    "    - [`chi2`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "    - [`f_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)\n",
    "    - [`mutual_info_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html)\n",
    "    \n",
    "**ВНИМАНИЕ: Не используйте функцию оценки регрессии с классификацией (и наоборот), так как в результате получите бесполезные результаты.**\n",
    "    \n",
    "Методы, основанные на F-тесте (F-test), оценивают степень линейной зависимости между двумя случайными величинами.\n",
    "\n",
    "С другой стороны, методы взаимной информации (mutual information) могут фиксировать любой вид статистической зависимости, но, будучи непараметрическими, они требуют большего количества выборок (samples) для точной оценки.\n",
    "\n",
    "Это можно рассматривать как этап предварительной обработки модели (estimator). \n",
    "\n",
    "\n",
    "Scikit-learn предоставляет процедуры выбора признаков как объекты, реализующие метод `transform`:\n",
    "\n",
    "- [sklearn.feature_selection.SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) удаляет все, кроме k признаков с наивысшими оценками.\n",
    "\n",
    "\n",
    "- [sklearn.feature_selection.SelectPercentile](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html) удаляет все признаки, кроме указанного пользователем наивысшего процента признаков, которые нужно оставить.\n",
    "\n",
    "\n",
    "- Использование общих одномерных статистических тестов для каждой функции: \n",
    "    - частота ложных срабатываний (False Positive Rate) [sklearn.feature_selection.SelectFpr](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html)\n",
    "    - частота ложных обнаружений (False Discovery Rate) [sklearn.feature_selection.SelectFdr](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr.html)\n",
    "    - Вероятность сделать одно или несколько ложных открытий или ошибок типа I при выполнении проверки нескольких гипотез (Family-Wise Error) [sklearn.feature_selection.SelectFwe](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html)\n",
    "\n",
    "\n",
    "- **[sklearn.feature_selection.GenericUnivariateSelect](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html)** позволяет выполнять одномерный выбор признаков с настраиваемой стратегией. Это позволяет выбрать лучшую одномерную стратегию выбора с помощью гиперпараметрического поискового оценщика (estimator), например, GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекурсивное устранение признаков (Recursive feature elimination)\n",
    "\n",
    "При наличии внешнего оценщика (external estimator), который присваивает веса признакам (например, коэффициентам линейной модели), цель рекурсивного исключения признаков ([sklearn.feature_selection.RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)) состоит в том, чтобы выбрать признаки путем рекурсивного рассмотрения все меньших и меньших наборов признаков.\n",
    "\n",
    "Во-первых, оценщик обучается на начальном наборе признаков, и важность каждого признака определяется либо через любой конкретный атрибут (например, `coef_`, `feature_importances_`), либо через вызываемый (своя собственная функция).\n",
    "\n",
    "Затем наименее важные признаки удаляются из текущего набора признаков.\n",
    "\n",
    "Эта процедура рекурсивно повторяется для сокращенного набора до тех пор, пока в конечном итоге не будет достигнуто желаемое количество признаков.\n",
    "\n",
    "**[sklearn.feature_selection.RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html) выполняет RFE с помощью cross-validation, чтобы найти оптимальное количество признаков.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор признаков с помощью SelectFromModel (Feature selection using SelectFromModel)\n",
    "\n",
    "[sklearn.feature_selection.SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) - это мета-преобразователь (transformer), который можно использовать вместе с любым оценщиком (estimator), который назначает важность каждого признака с помощью определенного атрибута (например, `coef_`, `feature_importances_`) или с помощью метода `important_getter`, вызываемого после обучения (fitting).\n",
    "\n",
    "Признаки считаются неважными и удаляются, если соответствующая важность значений признака ниже указанного порогового параметра. \n",
    "\n",
    "Помимо числового указания порога, существуют встроенные эвристики для поиска порога с использованием строкового аргумента.\n",
    "\n",
    "Доступные эвристики: `\"mean\"`, `\"median\"` и числа с плавающей запятой, такие как `\"0.1*mean\"`.\n",
    "\n",
    "В сочетании с пороговыми критериями можно использовать параметр `max_features`, чтобы установить ограничение на количество выбираемых признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Выбор признаков на основе L1 (L1-based feature selection)\n",
    "\n",
    "Линейные модели, с L1 регуляризацией, имеют разреженные результаты: многие из их оценочных коэффициентов равны нулю.\n",
    "\n",
    "Когда цель состоит в том, чтобы уменьшить размерность данных для использования с другим классификатором, их можно использовать вместе с [sklearn.feature_selection.SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) для выбора ненулевых коэффициентов.\n",
    "\n",
    "В частности, для этой цели можно использовать разреженные оценки: [sklearn.linear_model.Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) для регрессии, а также [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) и [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) для классификации.\n",
    "\n",
    "В [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) и [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) параметр `C` контролирует разреженность: чем меньше `C`, тем меньше выбирается признаков. \n",
    "\n",
    "При использовании [sklearn.linear_model.Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html), чем выше `alpha`, тем меньше признаков выбирается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор признаков на основе дерева (Tree-based feature selection)\n",
    "\n",
    "Модели на основе деревьев (см. [sklearn.tree](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) и лес деревьев в [sklearn.ensemble](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)) могут использоваться для вычисления важности признаков, которые могут использоваться для отбрасывания нерелевантных признаков (в сочетании с [sklearn.feature_selection.SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последовательный выбор признаков (Sequential Feature Selection)\n",
    "\n",
    "Последовательный выбор признаков (SFS) доступен в преобразователе [sklearn.feature_selection.SequentialFeatureSelector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector). SFS может быть использован как вперед, так и назад:\n",
    "\n",
    "- Forward-SFS (`direction=\"forward\"` - по умолчанию) - это [жадный алгоритм](https://ru.wikipedia.org/wiki/%D0%96%D0%B0%D0%B4%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC), который итеративно находит лучший новый признак для добавления к набору выбранных признаков. \n",
    "    - Сначала начинаем с нулевого признака и находим один признак, который максимизирует cross-validated score, когда модель обучается на этом единственном признаке. \n",
    "    - Как только этот первый признак выбран, повторяем процедуру, добавляя новый признак к набору выбранных признаков.\n",
    "    - Процедура останавливается, когда достигается желаемое количество выбранных признаков, как определено параметром `n_features_to_select`.\n",
    "    \n",
    "\n",
    "- Backward-SFS (`direction=\"backward\"`) следует той же идее, но работает в противоположном направлении: вместо того, чтобы начинать без признаков и жадно добавлять признаки, начинаем со всех признаков и жадно удаляем признаки из набора.\n",
    "\n",
    "Как правило, прямой (`direction=\"forward\"`) и обратный отбор (`direction=\"backward\"`) дают различные результаты. \n",
    "\n",
    "\n",
    "ВНИМАНИЕ: Одна модель может быть намного быстрее, чем другая, в зависимости от запрошенного количества выбранных признаков: если у есть 10 признаков и запрашиваем 7 выбранных признаков, для прямого выбора потребуется выполнить 7 итераций, а для обратного выбора потребуется выполнить только 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:computer-science-1]",
   "language": "python",
   "name": "conda-env-computer-science-1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
